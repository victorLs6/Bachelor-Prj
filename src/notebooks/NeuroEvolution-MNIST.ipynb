{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9e01b1",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee312f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab29bc9",
   "metadata": {},
   "source": [
    "MNIST data set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f586b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fba5e",
   "metadata": {},
   "source": [
    "creating SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6988dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn(layer_sizes, beta, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Create SNN with variable number of layers and improved architecture\n",
    "    \"\"\"\n",
    "    class SNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.layers = nn.ModuleList()\n",
    "            self.lif_layers = nn.ModuleList()\n",
    "            self.dropout_layers = nn.ModuleList()\n",
    "            \n",
    "            # Input layer\n",
    "            input_size = 28 * 28\n",
    "            for i, hidden_size in enumerate(layer_sizes):\n",
    "                self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "                self.lif_layers.append(snn.Leaky(beta=beta))\n",
    "                # Add dropout for regularization (except last layer)\n",
    "                if i < len(layer_sizes) - 1:\n",
    "                    self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "                else:\n",
    "                    self.dropout_layers.append(nn.Identity())\n",
    "                input_size = hidden_size\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(nn.Linear(input_size, 10))\n",
    "            self.lif_layers.append(snn.Leaky(beta=beta))\n",
    "\n",
    "        def forward(self, x, num_steps=15):  # Increased time steps\n",
    "            # Initialize membrane potentials for all layers\n",
    "            mem_states = [lif.init_leaky() for lif in self.lif_layers]\n",
    "            spk_out = 0\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                current_input = x.view(x.size(0), -1)\n",
    "                \n",
    "                # Forward through all layers\n",
    "                for i, (layer, lif, dropout) in enumerate(zip(self.layers, self.lif_layers, self.dropout_layers + [nn.Identity()])):\n",
    "                    current = layer(current_input)\n",
    "                    spike, mem_states[i] = lif(current, mem_states[i])\n",
    "                    if i < len(self.layers) - 1:  # Apply dropout except on output\n",
    "                        spike = dropout(spike)\n",
    "                    current_input = spike\n",
    "                \n",
    "                spk_out += current_input\n",
    "            \n",
    "            return spk_out / num_steps\n",
    "    \n",
    "    return SNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bf7b2",
   "metadata": {},
   "source": [
    "Creating mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual_with_depth():\n",
    "    \"\"\"Create individual with improved parameter ranges\"\"\"\n",
    "    num_hidden_layers = random.randint(2, 5)  # Reduced max depth for stability\n",
    "    \n",
    "    # Create layer sizes with tapering (wider to narrower)\n",
    "    layer_sizes = []\n",
    "    base_size = random.uniform(128, 256)  # Start with wider layers\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        # Gradually reduce size for deeper layers\n",
    "        reduction_factor = 0.7 ** i\n",
    "        size = max(32, base_size * reduction_factor + random.uniform(-20, 20))\n",
    "        layer_sizes.append(size)\n",
    "    \n",
    "    beta = random.uniform(0.7, 0.95)  # Narrower beta range\n",
    "    lr = random.uniform(0.001, 0.005)  # Narrower LR range\n",
    "    dropout = random.uniform(0.0, 0.2)  # Add dropout parameter\n",
    "    \n",
    "    individual = [num_hidden_layers] + layer_sizes + [beta, lr, dropout]\n",
    "    return individual\n",
    "\n",
    "def bounded_mutation_with_depth(individual, mu=0, sigma=0.15, indpb=0.15):\n",
    "    \"\"\"Improved mutation with smaller perturbations\"\"\"\n",
    "    num_layers = int(individual[0])\n",
    "    \n",
    "    # Mutate number of layers (lower probability)\n",
    "    if random.random() < 0.08:  # Reduced probability\n",
    "        new_num_layers = max(2, min(4, num_layers + random.choice([-1, 1])))\n",
    "        \n",
    "        if new_num_layers > num_layers:\n",
    "            # Add a new layer (smaller than previous)\n",
    "            prev_size = individual[num_layers] if num_layers > 0 else 128\n",
    "            new_layer_size = max(32, prev_size * 0.7 + random.uniform(-10, 10))\n",
    "            individual.insert(num_layers + 1, new_layer_size)\n",
    "        elif new_num_layers < num_layers:\n",
    "            individual.pop(num_layers)\n",
    "        \n",
    "        individual[0] = new_num_layers\n",
    "        num_layers = new_num_layers\n",
    "    \n",
    "    # Mutate layer sizes with smaller perturbations\n",
    "    for i in range(1, num_layers + 1):\n",
    "        if random.random() < indpb:\n",
    "            individual[i] += random.gauss(mu, sigma * 30)  # Smaller mutations\n",
    "            individual[i] = max(32, min(256, individual[i]))\n",
    "    \n",
    "    # Mutate beta\n",
    "    if random.random() < indpb:\n",
    "        individual[num_layers + 1] += random.gauss(mu, sigma * 0.05)\n",
    "        individual[num_layers + 1] = max(0.7, min(0.95, individual[num_layers + 1]))\n",
    "    \n",
    "    # Mutate learning rate\n",
    "    if random.random() < indpb:\n",
    "        individual[num_layers + 2] += random.gauss(mu, sigma * 0.001)\n",
    "        individual[num_layers + 2] = max(0.001, min(0.005, individual[num_layers + 2]))\n",
    "    \n",
    "    # Mutate dropout\n",
    "    if random.random() < indpb:\n",
    "        individual[num_layers + 3] += random.gauss(mu, sigma * 0.05)\n",
    "        individual[num_layers + 3] = max(0.0, min(0.2, individual[num_layers + 3]))\n",
    "    \n",
    "    return individual,\n",
    "\n",
    "def crossover_with_depth(ind1, ind2):\n",
    "    \"\"\"Improved crossover for variable-length individuals\"\"\"\n",
    "    num_layers1 = int(ind1[0])\n",
    "    num_layers2 = int(ind2[0])\n",
    "    \n",
    "    # Choose depth more intelligently (favor successful depths)\n",
    "    if hasattr(ind1, 'fitness') and hasattr(ind2, 'fitness'):\n",
    "        if ind1.fitness.valid and ind2.fitness.valid:\n",
    "            if ind1.fitness.values[0] > ind2.fitness.values[0]:\n",
    "                chosen_depth = num_layers1\n",
    "            else:\n",
    "                chosen_depth = num_layers2\n",
    "        else:\n",
    "            chosen_depth = random.choice([num_layers1, num_layers2])\n",
    "    else:\n",
    "        chosen_depth = random.choice([num_layers1, num_layers2])\n",
    "    \n",
    "    # Create new individuals\n",
    "    new_ind1 = [chosen_depth]\n",
    "    new_ind2 = [chosen_depth]\n",
    "    \n",
    "    # Blend layer sizes\n",
    "    for i in range(chosen_depth):\n",
    "        if i < num_layers1 and i < num_layers2:\n",
    "            # Blend existing layers\n",
    "            alpha = random.uniform(0.3, 0.7)  # Less extreme blending\n",
    "            size1 = alpha * ind1[1 + i] + (1 - alpha) * ind2[1 + i]\n",
    "            size2 = (1 - alpha) * ind1[1 + i] + alpha * ind2[1 + i]\n",
    "        elif i < num_layers1:\n",
    "            size1 = ind1[1 + i] + random.uniform(-10, 10)\n",
    "            size2 = random.uniform(32, 128)\n",
    "        elif i < num_layers2:\n",
    "            size1 = random.uniform(32, 128)\n",
    "            size2 = ind2[1 + i] + random.uniform(-10, 10)\n",
    "        else:\n",
    "            size1 = random.uniform(64, 128)\n",
    "            size2 = random.uniform(64, 128)\n",
    "        \n",
    "        new_ind1.append(max(32, min(256, size1)))\n",
    "        new_ind2.append(max(32, min(256, size2)))\n",
    "    \n",
    "    # Blend other parameters\n",
    "    alpha = random.uniform(0.3, 0.7)\n",
    "    beta1 = alpha * ind1[num_layers1 + 1] + (1 - alpha) * ind2[num_layers2 + 1]\n",
    "    beta2 = (1 - alpha) * ind1[num_layers1 + 1] + alpha * ind2[num_layers2 + 1]\n",
    "    lr1 = alpha * ind1[num_layers1 + 2] + (1 - alpha) * ind2[num_layers2 + 2]\n",
    "    lr2 = (1 - alpha) * ind1[num_layers1 + 2] + alpha * ind2[num_layers2 + 2]\n",
    "    dropout1 = alpha * ind1[num_layers1 + 3] + (1 - alpha) * ind2[num_layers2 + 3]\n",
    "    dropout2 = (1 - alpha) * ind1[num_layers1 + 3] + alpha * ind2[num_layers2 + 3]\n",
    "    \n",
    "    new_ind1.extend([beta1, lr1, dropout1])\n",
    "    new_ind2.extend([beta2, lr2, dropout2])\n",
    "    \n",
    "    ind1[:] = new_ind1\n",
    "    ind2[:] = new_ind2\n",
    "    \n",
    "    return ind1, ind2\n",
    "\n",
    "def evaluate_model_with_depth(individual):\n",
    "    \"\"\"Improved evaluation with more training and better metrics\"\"\"\n",
    "    try:\n",
    "        # Parse individual\n",
    "        num_layers = int(individual[0])\n",
    "        layer_sizes = [max(32, min(256, int(round(individual[i])))) \n",
    "                       for i in range(1, num_layers + 1)]\n",
    "        beta = max(0.7, min(0.95, float(individual[num_layers + 1])))\n",
    "        lr = max(0.001, min(0.005, float(individual[num_layers + 2])))\n",
    "        dropout = max(0.0, min(0.2, float(individual[num_layers + 3])))\n",
    "        \n",
    "        model = create_snn(layer_sizes, beta, dropout)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Extended training loop\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Train on more batches based on network depth\n",
    "            max_batches = min(50, 30 + num_layers * 5)\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "        # More comprehensive evaluation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        eval_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                eval_loss += loss.item()\n",
    "                \n",
    "                _, pred = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += (pred == targets).sum().item()\n",
    "                \n",
    "                # Evaluate on more samples for better accuracy\n",
    "                if total >= 2000:  # Increased from 1000\n",
    "                    break\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        avg_eval_loss = eval_loss / min(batch_idx + 1, len(test_loader))\n",
    "        \n",
    "        # Penalize overfitting and complexity\n",
    "        complexity_penalty = num_layers * 0.001 + sum(layer_sizes) * 0.00001\n",
    "        overfitting_penalty = max(0, avg_train_loss - avg_eval_loss) * 0.1\n",
    "        \n",
    "        final_fitness = accuracy\n",
    "        \n",
    "        return (final_fitness,)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating individual: {e}\")\n",
    "        return (0.0,)  # Return poor fitness for failed evaluations\n",
    "\n",
    "def parse_individual(individual):\n",
    "    \"\"\"Helper function to parse individual parameters\"\"\"\n",
    "    num_layers = int(individual[0])\n",
    "    layer_sizes = [int(round(individual[i])) for i in range(1, num_layers + 1)]\n",
    "    beta = individual[num_layers + 1]\n",
    "    lr = individual[num_layers + 2]\n",
    "    dropout = individual[num_layers + 3]\n",
    "    return num_layers, layer_sizes, beta, lr, dropout\n",
    "\n",
    "def print_best_individual(generation, best_individual, fitness):\n",
    "    \"\"\"Print detailed information about the best individual\"\"\"\n",
    "    num_layers, layer_sizes, beta, lr, dropout = parse_individual(best_individual)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATION {generation} - BEST INDIVIDUAL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Fitness: {fitness:.6f}\")\n",
    "    print(f\"Architecture: 784 → {' → '.join(map(str, layer_sizes))} → 10\")\n",
    "    print(f\"Beta (β): {beta:.4f}\")\n",
    "    print(f\"Learning Rate: {lr:.6f}\")\n",
    "    print(f\"Dropout Rate: {dropout:.4f}\")\n",
    "    print(f\"Total Parameters: ~{sum([784] + layer_sizes + [10]) * sum(layer_sizes + [10]):,}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcb91c",
   "metadata": {},
   "source": [
    "Creating plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ff358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution_progress(logbook, generation_best):\n",
    "    \"\"\"Plot evolution progress with multiple metrics\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Extract data\n",
    "    generations = [record['gen'] for record in logbook]\n",
    "    max_fitness = [record['max'] for record in logbook]\n",
    "    avg_fitness = [record['avg'] for record in logbook]\n",
    "    min_fitness = [record['min'] for record in logbook]\n",
    "    std_fitness = [record['std'] for record in logbook]\n",
    "    \n",
    "    # Plot 1: Fitness Evolution\n",
    "    ax1.plot(generations, max_fitness, 'r-', linewidth=2, label='Max Fitness', marker='o')\n",
    "    ax1.plot(generations, avg_fitness, 'b-', linewidth=2, label='Avg Fitness', marker='s')\n",
    "    ax1.fill_between(generations, \n",
    "                     [avg - std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     [avg + std for avg, std in zip(avg_fitness, std_fitness)],\n",
    "                     alpha=0.3, color='blue')\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness')\n",
    "    ax1.set_title('Evolution Progress - Fitness Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Best Fitness Trajectory\n",
    "    best_gens = [item[0] for item in generation_best]\n",
    "    best_fits = [item[2] for item in generation_best]\n",
    "    ax2.plot(best_gens, best_fits, 'g-', linewidth=3, marker='*', markersize=8)\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Best Fitness')\n",
    "    ax2.set_title('Best Individual Fitness Trajectory')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate improvements\n",
    "    for i in range(1, len(best_fits)):\n",
    "        if best_fits[i] > best_fits[i-1]:\n",
    "            ax2.annotate(f'↑ {best_fits[i]:.4f}', \n",
    "                        xy=(best_gens[i], best_fits[i]),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Plot 3: Fitness Distribution\n",
    "    try:\n",
    "        final_gen_data = [ind.fitness.values[0] for ind in logbook[-1]['population'] \n",
    "                          if hasattr(ind, 'fitness') and ind.fitness.valid]\n",
    "    except KeyError:\n",
    "        final_gen_data = [max_fitness[-1], avg_fitness[-1], min_fitness[-1]]\n",
    "    \n",
    "    ax3.hist(final_gen_data, bins=15, alpha=0.7, color='purple', edgecolor='black')\n",
    "    ax3.axvline(np.mean(final_gen_data), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(final_gen_data):.4f}')\n",
    "    ax3.set_xlabel('Fitness')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Final Generation Fitness Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Convergence Analysis\n",
    "    improvement_rate = []\n",
    "    for i in range(1, len(max_fitness)):\n",
    "        rate = max_fitness[i] - max_fitness[i-1]\n",
    "        improvement_rate.append(rate)\n",
    "    \n",
    "    ax4.plot(generations[1:], improvement_rate, 'orange', linewidth=2, marker='d')\n",
    "    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax4.set_xlabel('Generation')\n",
    "    ax4.set_ylabel('Fitness Improvement')\n",
    "    ax4.set_title('Convergence Rate (Generation-to-Generation Improvement)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evolution_progress.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_architecture_evolution(generation_best):\n",
    "    \"\"\"Plot architecture evolution over generations\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Extract architecture data\n",
    "    generations = []\n",
    "    num_layers_list = []\n",
    "    layer_sizes_by_gen = []\n",
    "    total_params = []\n",
    "    \n",
    "    for gen, individual, fitness in generation_best:\n",
    "        generations.append(gen)\n",
    "        num_layers, layer_sizes, beta, lr, dropout = parse_individual(individual)\n",
    "        num_layers_list.append(num_layers)\n",
    "        layer_sizes_by_gen.append(layer_sizes)\n",
    "        \n",
    "        # Calculate approximate parameter count\n",
    "        params = 784 * layer_sizes[0]  # Input to first hidden\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            params += layer_sizes[i] * layer_sizes[i + 1]\n",
    "        params += layer_sizes[-1] * 10  # Last hidden to output\n",
    "        total_params.append(params)\n",
    "    \n",
    "    # Plot 1: Number of Layers Evolution\n",
    "    ax1.plot(generations, num_layers_list, 'b-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Number of Hidden Layers')\n",
    "    ax1.set_title('Architecture Depth Evolution')\n",
    "    ax1.set_ylim(1.5, 4.5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate layer changes\n",
    "    for i in range(1, len(num_layers_list)):\n",
    "        if num_layers_list[i] != num_layers_list[i-1]:\n",
    "            change = \"+\" if num_layers_list[i] > num_layers_list[i-1] else \"-\"\n",
    "            ax1.annotate(f'{change}Layer', \n",
    "                        xy=(generations[i], num_layers_list[i]),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue'),\n",
    "                        arrowprops=dict(arrowstyle='->'))\n",
    "    \n",
    "    # Plot 2: Layer Sizes Heatmap\n",
    "    max_layers = max(len(sizes) for sizes in layer_sizes_by_gen)\n",
    "    heatmap_data = np.zeros((len(generations), max_layers))\n",
    "    \n",
    "    for i, sizes in enumerate(layer_sizes_by_gen):\n",
    "        for j, size in enumerate(sizes):\n",
    "            heatmap_data[i, j] = size\n",
    "    \n",
    "    im = ax2.imshow(heatmap_data.T, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Layer Index')\n",
    "    ax2.set_title('Layer Sizes Evolution Heatmap')\n",
    "    ax2.set_xticks(range(0, len(generations), max(1, len(generations)//10)))\n",
    "    ax2.set_xticklabels(generations[::max(1, len(generations)//10)])\n",
    "    plt.colorbar(im, ax=ax2, label='Layer Size')\n",
    "    \n",
    "    # Plot 3: Total Parameters Evolution\n",
    "    ax3.plot(generations, total_params, 'r-s', linewidth=2, markersize=6)\n",
    "    ax3.set_xlabel('Generation')\n",
    "    ax3.set_ylabel('Total Parameters')\n",
    "    ax3.set_title('Model Complexity Evolution')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Format y-axis to show K/M notation\n",
    "    ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K' if x < 1000000 else f'{x/1000000:.1f}M'))\n",
    "    \n",
    "    # Plot 4: Architecture Visualization for Best Individual\n",
    "    best_individual = max(generation_best, key=lambda x: x[2])\n",
    "    _, best_layer_sizes, _, _, _ = parse_individual(best_individual[1])\n",
    "    \n",
    "    # Create network diagram\n",
    "    ax4.set_xlim(0, 10)\n",
    "    ax4.set_ylim(0, 10)\n",
    "    \n",
    "    # Draw layers\n",
    "    layer_positions = np.linspace(1, 9, len(best_layer_sizes) + 2)  # +2 for input and output\n",
    "    all_sizes = [784] + best_layer_sizes + [10]\n",
    "    \n",
    "    max_neurons = max(all_sizes)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(all_sizes)))\n",
    "    \n",
    "    for i, (pos, size, color) in enumerate(zip(layer_positions, all_sizes, colors)):\n",
    "        # Scale rectangle height based on layer size\n",
    "        height = (size / max_neurons) * 6 + 1\n",
    "        y_center = 5\n",
    "        \n",
    "        rect = Rectangle((pos - 0.3, y_center - height/2), 0.6, height, \n",
    "                        facecolor=color, edgecolor='black', alpha=0.7)\n",
    "        ax4.add_patch(rect)\n",
    "        \n",
    "        # Add labels\n",
    "        if i == 0:\n",
    "            label = f'Input\\n{size}'\n",
    "        elif i == len(all_sizes) - 1:\n",
    "            label = f'Output\\n{size}'\n",
    "        else:\n",
    "            label = f'H{i}\\n{size}'\n",
    "        \n",
    "        ax4.text(pos, y_center - height/2 - 0.5, label, ha='center', va='top', fontsize=8)\n",
    "        \n",
    "        # Draw connections\n",
    "        if i < len(layer_positions) - 1:\n",
    "            ax4.arrow(pos + 0.3, y_center, 0.4, 0, head_width=0.1, \n",
    "                     head_length=0.1, fc='gray', ec='gray', alpha=0.5)\n",
    "    \n",
    "    ax4.set_title(f'Best Architecture (Gen {best_individual[0]}, Fitness: {best_individual[2]:.4f})')\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_yticks([])\n",
    "    ax4.spines['top'].set_visible(False)\n",
    "    ax4.spines['right'].set_visible(False)\n",
    "    ax4.spines['bottom'].set_visible(False)\n",
    "    ax4.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('architecture_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_beta_evolution(generation_best):\n",
    "    \"\"\"Plot beta parameter evolution and analysis\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Extract parameter data\n",
    "    generations = []\n",
    "    betas = []\n",
    "    learning_rates = []\n",
    "    dropouts = []\n",
    "    fitnesses = []\n",
    "    \n",
    "    for gen, individual, fitness in generation_best:\n",
    "        generations.append(gen)\n",
    "        num_layers, layer_sizes, beta, lr, dropout = parse_individual(individual)\n",
    "        betas.append(beta)\n",
    "        learning_rates.append(lr)\n",
    "        dropouts.append(dropout)\n",
    "        fitnesses.append(fitness)\n",
    "    \n",
    "    # Plot 1: Beta Evolution\n",
    "    ax1.plot(generations, betas, 'purple', linewidth=3, marker='o', markersize=8)\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Beta Value')\n",
    "    ax1.set_title('Beta Parameter Evolution')\n",
    "    ax1.set_ylim(0.65, 1.0)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight significant changes\n",
    "    for i in range(1, len(betas)):\n",
    "        if abs(betas[i] - betas[i-1]) > 0.05:\n",
    "            ax1.annotate(f'β={betas[i]:.3f}', \n",
    "                        xy=(generations[i], betas[i]),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                        arrowprops=dict(arrowstyle='->'))\n",
    "    \n",
    "    # Plot 2: Learning Rate Evolution\n",
    "    ax2.plot(generations, learning_rates, 'green', linewidth=3, marker='s', markersize=6)\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title('Learning Rate Evolution')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    # Plot 3: Parameter Correlation with Fitness\n",
    "    # Create scatter plot with color coding for generations\n",
    "    scatter = ax3.scatter(betas, fitnesses, c=generations, cmap='viridis', \n",
    "                         s=100, alpha=0.7, edgecolors='black')\n",
    "    ax3.set_xlabel('Beta Value')\n",
    "    ax3.set_ylabel('Fitness')\n",
    "    ax3.set_title('Beta vs Fitness Correlation')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax3, label='Generation')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(betas, fitnesses, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax3.plot(betas, p(betas), \"r--\", alpha=0.8, linewidth=2, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot 4: All Parameters Over Time\n",
    "    # Normalize parameters to [0, 1] for comparison\n",
    "    norm_betas = [(b - min(betas)) / (max(betas) - min(betas)) for b in betas]\n",
    "    norm_lrs = [(lr - min(learning_rates)) / (max(learning_rates) - min(learning_rates)) for lr in learning_rates]\n",
    "    norm_dropouts = [(d - min(dropouts)) / (max(dropouts) - min(dropouts)) if max(dropouts) > min(dropouts) else 0.5 for d in dropouts]\n",
    "    \n",
    "    ax4.plot(generations, norm_betas, 'purple', linewidth=2, marker='o', label='Beta (normalized)')\n",
    "    ax4.plot(generations, norm_lrs, 'green', linewidth=2, marker='s', label='Learning Rate (normalized)')\n",
    "    ax4.plot(generations, norm_dropouts, 'orange', linewidth=2, marker='^', label='Dropout (normalized)')\n",
    "    \n",
    "    ax4.set_xlabel('Generation')\n",
    "    ax4.set_ylabel('Normalized Parameter Value')\n",
    "    ax4.set_title('All Parameters Evolution (Normalized)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('beta_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_comprehensive_summary_plot(logbook, generation_best):\n",
    "    \"\"\"Create a comprehensive summary plot with key metrics\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Extract all data\n",
    "    generations = [record['gen'] for record in logbook]\n",
    "    max_fitness = [record['max'] for record in logbook]\n",
    "    avg_fitness = [record['avg'] for record in logbook]\n",
    "    \n",
    "    gen_data = []\n",
    "    fitness_data = []\n",
    "    beta_data = []\n",
    "    lr_data = []\n",
    "    layers_data = []\n",
    "    param_counts = []\n",
    "    \n",
    "    for gen, individual, fitness in generation_best:\n",
    "        gen_data.append(gen)\n",
    "        fitness_data.append(fitness)\n",
    "        num_layers, layer_sizes, beta, lr, dropout = parse_individual(individual)\n",
    "        beta_data.append(beta)\n",
    "        lr_data.append(lr)\n",
    "        layers_data.append(num_layers)\n",
    "        \n",
    "        # Calculate parameters\n",
    "        params = 784 * layer_sizes[0]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            params += layer_sizes[i] * layer_sizes[i + 1]\n",
    "        params += layer_sizes[-1] * 10\n",
    "        param_counts.append(params)\n",
    "    \n",
    "    # Main fitness plot (spans 2 columns)\n",
    "    ax_main = fig.add_subplot(gs[0, :2])\n",
    "    ax_main.plot(generations, max_fitness, 'r-', linewidth=3, label='Population Max', marker='o', markersize=6)\n",
    "    ax_main.plot(generations, avg_fitness, 'b-', linewidth=2, label='Population Avg', alpha=0.7)\n",
    "    ax_main.plot(gen_data, fitness_data, 'g-', linewidth=3, label='Best Individual', marker='*', markersize=10)\n",
    "    ax_main.set_xlabel('Generation', fontsize=12)\n",
    "    ax_main.set_ylabel('Fitness', fontsize=12)\n",
    "    ax_main.set_title('Evolution Summary - Fitness Progression', fontsize=14, fontweight='bold')\n",
    "    ax_main.legend(fontsize=11)\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best individual info (right column, top)\n",
    "    ax_info = fig.add_subplot(gs[0, 2])\n",
    "    ax_info.axis('off')\n",
    "    best_idx = fitness_data.index(max(fitness_data))\n",
    "    best_gen = gen_data[best_idx]\n",
    "    best_fitness = fitness_data[best_idx]\n",
    "    best_individual = generation_best[best_idx][1]\n",
    "    \n",
    "    _, best_layers, best_beta, best_lr, best_dropout = parse_individual(best_individual)\n",
    "    \n",
    "    info_text = f\"\"\"BEST INDIVIDUAL\n",
    "    \n",
    "Generation: {best_gen}\n",
    "Fitness: {best_fitness:.6f}\n",
    "\n",
    "Architecture:\n",
    "• Layers: {len(best_layers)}\n",
    "• Sizes: {best_layers}\n",
    "• Parameters: ~{param_counts[best_idx]:,}\n",
    "\n",
    "Parameters:\n",
    "• Beta: {best_beta:.4f}\n",
    "• Learning Rate: {best_lr:.6f}\n",
    "• Dropout: {best_dropout:.4f}\"\"\"\n",
    "    \n",
    "    ax_info.text(0.05, 0.95, info_text, transform=ax_info.transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # Architecture depth evolution\n",
    "    ax_depth = fig.add_subplot(gs[1, 0])\n",
    "    ax_depth.plot(gen_data, layers_data, 'purple', linewidth=2, marker='s', markersize=6)\n",
    "    ax_depth.set_xlabel('Generation')\n",
    "    ax_depth.set_ylabel('Number of Layers')\n",
    "    ax_depth.set_title('Architecture Depth')\n",
    "    ax_depth.grid(True, alpha=0.3)\n",
    "    ax_depth.set_ylim(1.5, 4.5)\n",
    "    \n",
    "    # Parameter count evolution\n",
    "    ax_params = fig.add_subplot(gs[1, 1])\n",
    "    ax_params.plot(gen_data, param_counts, 'orange', linewidth=2, marker='^', markersize=6)\n",
    "    ax_params.set_xlabel('Generation')\n",
    "    ax_params.set_ylabel('Parameters')\n",
    "    ax_params.set_title('Model Complexity')\n",
    "    ax_params.grid(True, alpha=0.3)\n",
    "    ax_params.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K'))\n",
    "    \n",
    "    # Beta evolution\n",
    "    ax_beta = fig.add_subplot(gs[1, 2])\n",
    "    ax_beta.plot(gen_data, beta_data, 'red', linewidth=2, marker='o', markersize=6)\n",
    "    ax_beta.set_xlabel('Generation')\n",
    "    ax_beta.set_ylabel('Beta Value')\n",
    "    ax_beta.set_title('Beta Parameter')\n",
    "    ax_beta.grid(True, alpha=0.3)\n",
    "    ax_beta.set_ylim(0.65, 1.0)\n",
    "    \n",
    "    # Performance vs complexity scatter\n",
    "    ax_scatter = fig.add_subplot(gs[2, 0])\n",
    "    scatter = ax_scatter.scatter(param_counts, fitness_data, c=gen_data, cmap='viridis', \n",
    "                               s=100, alpha=0.7, edgecolors='black')\n",
    "    ax_scatter.set_xlabel('Parameter Count')\n",
    "    ax_scatter.set_ylabel('Fitness')\n",
    "    ax_scatter.set_title('Performance vs Complexity')\n",
    "    ax_scatter.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax_scatter, label='Generation')\n",
    "    \n",
    "    # Learning rate evolution\n",
    "    ax_lr = fig.add_subplot(gs[2, 1])\n",
    "    ax_lr.plot(gen_data, lr_data, 'green', linewidth=2, marker='d', markersize=6)\n",
    "    ax_lr.set_xlabel('Generation')\n",
    "    ax_lr.set_ylabel('Learning Rate')\n",
    "    ax_lr.set_title('Learning Rate Evolution')\n",
    "    ax_lr.grid(True, alpha=0.3)\n",
    "    ax_lr.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    # Improvement rate\n",
    "    ax_improve = fig.add_subplot(gs[2, 2])\n",
    "    improvement_rate = [0]  # First generation has no improvement\n",
    "    for i in range(1, len(fitness_data)):\n",
    "        improvement_rate.append(fitness_data[i] - fitness_data[i-1])\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'green' for x in improvement_rate[1:]]\n",
    "    ax_improve.bar(gen_data[1:], improvement_rate[1:], color=colors, alpha=0.7)\n",
    "    ax_improve.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax_improve.set_xlabel('Generation')\n",
    "    ax_improve.set_ylabel('Fitness Improvement')\n",
    "    ax_improve.set_title('Generation-to-Generation Progress')\n",
    "    ax_improve.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final statistics summary\n",
    "    ax_stats = fig.add_subplot(gs[3, :])\n",
    "    ax_stats.axis('off')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_improvement = fitness_data[-1] - fitness_data[0] if len(fitness_data) > 1 else 0\n",
    "    avg_improvement = total_improvement / len(fitness_data) if len(fitness_data) > 1 else 0\n",
    "    best_improvement_gen = improvement_rate.index(max(improvement_rate[1:])) + 1 if len(improvement_rate) > 1 else 0\n",
    "    \n",
    "    convergence_gen = len(fitness_data)\n",
    "    for i in range(len(fitness_data) - 5, 0, -1):\n",
    "        if abs(fitness_data[i] - fitness_data[-1]) > 0.001:\n",
    "            convergence_gen = i + 5\n",
    "            break\n",
    "    \n",
    "    stats_text = f\"\"\"EVOLUTION STATISTICS\n",
    "    \n",
    "Total Generations: {len(fitness_data)}                    Total Improvement: {total_improvement:.6f}                    Average per Generation: {avg_improvement:.6f}\n",
    "    \n",
    "Best Generation: {best_gen}                    Convergence: ~Gen {convergence_gen}                    Best Improvement: Gen {best_improvement_gen} (+{max(improvement_rate[1:]) if len(improvement_rate) > 1 else 0:.6f})\n",
    "\n",
    "Architecture Range: {min(layers_data)}-{max(layers_data)} layers          Parameter Range: {min(param_counts):,}-{max(param_counts):,}          Beta Range: {min(beta_data):.3f}-{max(beta_data):.3f}\"\"\"\n",
    "    \n",
    "    ax_stats.text(0.5, 0.5, stats_text, transform=ax_stats.transAxes, fontsize=12,\n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 bbox=dict(boxstyle='round,pad=1', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('SNN Evolution Comprehensive Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.savefig('comprehensive_evolution_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdede3",
   "metadata": {},
   "source": [
    "Set up  DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61141b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def create_individual():\n",
    "    \"\"\"Create an individual and wrap it in creator.Individual\"\"\"\n",
    "    ind_data = create_individual_with_depth()\n",
    "    return creator.Individual(ind_data)\n",
    "\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_model_with_depth)\n",
    "toolbox.register(\"mate\", crossover_with_depth)\n",
    "toolbox.register(\"mutate\", bounded_mutation_with_depth, mu=0, sigma=0.1, indpb=0.15)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb514c",
   "metadata": {},
   "source": [
    "Running the evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evolution...\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "GENERATION 0 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.819501\n",
      "Architecture: 784 → 237 → 170 → 10\n",
      "Beta (β): 0.7480\n",
      "Learning Rate: 0.003974\n",
      "Dropout Rate: 0.0105\n",
      "Total Parameters: ~500,817\n",
      "============================================================\n",
      "\n",
      "Gen 0 Stats:\n",
      " gen\tnevals\tavg     \tmax     \tmin      \tstd     \n",
      "0  \t400   \t0.491303\t0.819501\t0.0690844\t0.224451\n",
      "\n",
      "--- Processing Generation 1 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 1 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.827071\n",
      "Architecture: 784 → 244 → 160 → 10\n",
      "Beta (β): 0.7997\n",
      "Learning Rate: 0.003889\n",
      "Dropout Rate: 0.1736\n",
      "Total Parameters: ~495,972\n",
      "============================================================\n",
      "Gen 1 \n",
      "Stats: 1  \t266   \t0.725701\t0.827071\t0.385623 \t0.0836032\n",
      "\n",
      "--- Processing Generation 2 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 2 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.828989\n",
      "Architecture: 784 → 252 → 173 → 10\n",
      "Beta (β): 0.9322\n",
      "Learning Rate: 0.004020\n",
      "Dropout Rate: 0.0393\n",
      "Total Parameters: ~530,265\n",
      "============================================================\n",
      "Gen 2 \n",
      "Stats: 2  \t276   \t0.777489\t0.828989\t0.706    \t0.0257494\n",
      "\n",
      "--- Processing Generation 3 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 3 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.831121\n",
      "Architecture: 784 → 217 → 149 → 10\n",
      "Beta (β): 0.8325\n",
      "Learning Rate: 0.003746\n",
      "Dropout Rate: 0.0838\n",
      "Total Parameters: ~436,160\n",
      "============================================================\n",
      "Gen 3 \n",
      "Stats: 3  \t259   \t0.785405\t0.831121\t0.721391 \t0.0232898\n",
      "\n",
      "--- Processing Generation 4 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 4 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 4 \n",
      "Stats: 4  \t292   \t0.788202\t0.840294\t0.736134 \t0.0234483\n",
      "\n",
      "--- Processing Generation 5 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 5 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 5 \n",
      "Stats: 5  \t300   \t0.787942\t0.840294\t0.727993 \t0.0236934\n",
      "\n",
      "--- Processing Generation 6 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 6 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 6 \n",
      "Stats: 6  \t283   \t0.791218\t0.840294\t0.731353 \t0.0231459\n",
      "\n",
      "--- Processing Generation 7 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 7 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 7 \n",
      "Stats: 7  \t279   \t0.79347 \t0.840294\t0.728619 \t0.0238895\n",
      "\n",
      "--- Processing Generation 8 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 8 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 8 \n",
      "Stats: 8  \t293   \t0.791268\t0.840294\t0.731631 \t0.0250147\n",
      "\n",
      "--- Processing Generation 9 ---\n"
     ]
    }
   ],
   "source": [
    "def run_evolution():\n",
    "    \"\"\"Enhanced evolution with detailed best individual tracking and plotting\"\"\"\n",
    "    pop_size = 50\n",
    "    pop = toolbox.population(n=pop_size)\n",
    "    hof = tools.HallOfFame(pop_size/10)\n",
    "    \n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"std\", np.std)\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + stats.fields\n",
    "\n",
    "    # Store best individuals from each generation\n",
    "    generation_best = []\n",
    "    # Store populations for each generation\n",
    "    populations = []\n",
    "\n",
    "    print(\"Starting Evolution...\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Initial evaluation\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    hof.update(pop)\n",
    "    populations.append(pop)  # Store initial population\n",
    "    \n",
    "    # Find and display best individual of generation 0\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    generation_best.append((0, best_ind.copy(), best_ind.fitness.values[0]))\n",
    "    print_best_individual(0, best_ind, best_ind.fitness.values[0])\n",
    "    \n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, nevals=len(pop), **record)\n",
    "    print(f\"\\nGen 0 Stats:\\n {logbook.stream}\")\n",
    "    \n",
    "    # Evolution loop\n",
    "    for gen in range(1, 31):\n",
    "        print(f\"\\n--- Processing Generation {gen} ---\")\n",
    "        \n",
    "        # Selection and reproduction\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # Crossover\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < 0.6:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        \n",
    "        # Mutation\n",
    "        for mutant in offspring:\n",
    "            if random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluation\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Elitism: Keep best individuals\n",
    "        pop[:] = tools.selBest(offspring + list(hof), len(pop))\n",
    "        \n",
    "        hof.update(pop)\n",
    "        populations.append(pop.copy())  # Store current population\n",
    "        \n",
    "        # Find and display best individual of current generation\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        generation_best.append((gen, best_ind.copy(), best_ind.fitness.values[0]))\n",
    "        print_best_individual(gen, best_ind, best_ind.fitness.values[0])\n",
    "        \n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        print(f\"Gen {gen} \\nStats: {logbook.stream}\")\n",
    "        \n",
    "        # Early stopping if no improvement\n",
    "        if gen > 10 and logbook[-1]['max'] == logbook[-5]['max']:\n",
    "            print(f\"\\nEarly stopping at generation {gen} due to no improvement\")\n",
    "            break\n",
    "    \n",
    "    # Add population information to logbook\n",
    "    for i, entry in enumerate(logbook):\n",
    "        entry['population'] = populations[i]\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVOLUTION COMPLETE - FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print overall best\n",
    "    best_overall = max(generation_best, key=lambda x: x[2])\n",
    "    best_gen, best_ind, best_fitness = best_overall\n",
    "    \n",
    "    print(f\"\\nOVERALL BEST INDIVIDUAL (from Generation {best_gen}):\")\n",
    "    print_best_individual(\"FINAL\", best_ind, best_fitness)\n",
    "    \n",
    "    # Print evolution summary\n",
    "    print(f\"\\nEvolution Summary:\")\n",
    "    print(f\"- Total Generations: {gen}\")\n",
    "    print(f\"- Population Size: {pop_size}\")\n",
    "    print(f\"- Best Fitness Achieved: {best_fitness:.6f}\")\n",
    "    print(f\"- Best Found in Generation: {best_gen}\")\n",
    "    \n",
    "    # Generate all plots\n",
    "    print(\"\\nGenerating plots...\")\n",
    "    \n",
    "    print(\"1. Evolution Progress Plot...\")\n",
    "    plot_evolution_progress(logbook, generation_best)\n",
    "    \n",
    "    print(\"2. Architecture Evolution Plot...\")\n",
    "    plot_architecture_evolution(generation_best)\n",
    "    \n",
    "    print(\"3. Beta Evolution Plot...\")\n",
    "    plot_beta_evolution(generation_best)\n",
    "    \n",
    "    print(\"4. Comprehensive Summary Plot...\")\n",
    "    create_comprehensive_summary_plot(logbook, generation_best)\n",
    "    \n",
    "    print(\"All plots saved successfully!\")\n",
    "    \n",
    "    return pop, logbook, hof, generation_best\n",
    "if __name__ == '__main__':\n",
    "    population, logbook, hall_of_fame, best_individuals = run_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
