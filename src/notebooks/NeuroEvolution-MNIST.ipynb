{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9e01b1",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee312f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import snntorch as snn\n",
    "import torch    \n",
    "import torchvision\n",
    "from deap import base, creator, tools\n",
    "from rich.console import Console\n",
    "from rich.traceback import install\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab29bc9",
   "metadata": {},
   "source": [
    "MNIST data set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "install(show_locals=True)\n",
    "console = Console(width=180)\n",
    "RNG = np.random.default_rng()\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "backprop_df = pd.read_csv(\"src/notebooks/accuracy_log.csv\")\n",
    "backprop_accuracies = backprop_df[\"accuracy\"].tolist()\n",
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "POP_SIZE = 12\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fba5e",
   "metadata": {},
   "source": [
    "creating SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn(layer_sizes, beta, dropout_rate=0.1):\n",
    "\n",
    "    class SNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.lif_layers = nn.ModuleList()\n",
    "            self.dropout_layers = nn.ModuleList()\n",
    "\n",
    "            input_size = 28 * 28\n",
    "            num_of_classes = 10\n",
    "\n",
    "            for i, hidden_size in enumerate(layer_sizes):\n",
    "                self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "                self.lif_layers.append(snn.Leaky(beta=beta))\n",
    "\n",
    "                if i < len(layer_sizes) - 1:\n",
    "                    self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "                else:\n",
    "                    self.dropout_layers.append(nn.Identity())\n",
    "\n",
    "                input_size = hidden_size\n",
    "\n",
    "            self.layers.append(nn.Linear(input_size, num_of_classes))\n",
    "\n",
    "            self.apply(self._init_weights)\n",
    "\n",
    "        def _init_weights(self, m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=0.8)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "        def forward(self, x, num_steps=15):  \n",
    "            mem_states = [lif.init_leaky() for lif in self.lif_layers]\n",
    "            spk_out = 0\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                current_input = x.view(x.size(0), -1)\n",
    "\n",
    "                for i, (layer, lif, dropout) in enumerate(\n",
    "                    zip(\n",
    "                        self.layers,\n",
    "                        self.lif_layers,\n",
    "                        self.dropout_layers + [nn.Identity()],\n",
    "                    )\n",
    "                ):\n",
    "                    current = layer(current_input)\n",
    "                    spike, mem_states[i] = lif(current, mem_states[i])\n",
    "                    if (\n",
    "                        i < len(self.layers) - 1\n",
    "                    ):  \n",
    "                        spike = dropout(spike)\n",
    "                    current_input = spike\n",
    "\n",
    "                spk_out += current_input\n",
    "\n",
    "            return spk_out / num_steps\n",
    "\n",
    "    return SNNModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bf7b2",
   "metadata": {},
   "source": [
    "Creating mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual_with_depth():\n",
    "    \"\"\"Create individual with improved parameter ranges and depth diversity.\"\"\"\n",
    "    # Bias toward mid-range depths (3-6 layers) but allow full range\n",
    "    depth_weights = [0.05, 0.05, 0.05, 0.20, 0.25, 0.20, 0.10, 0.05, 0.05]  # Favor 4-6 layers more strongly\n",
    "    num_hidden_layers = np.random.choice(range(1, 10), p=depth_weights) \n",
    "\n",
    "    # Create layer sizes with tapering (wider to narrower)\n",
    "    layer_sizes = []\n",
    "    base_size = 512\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        # Gradually reduce size for deeper layers\n",
    "        reduction_factor = 0.85**i\n",
    "        size = max(64, int(base_size * reduction_factor + RNG.uniform(-30, 30)))\n",
    "        layer_sizes.append(size)\n",
    "\n",
    "    # Depth-adaptive parameters\n",
    "    beta = RNG.uniform(0.6, 0.90)  # Wider beta range\n",
    "    \n",
    "    # Adjust learning rate based on depth - deeper networks need lower LR\n",
    "    base_lr = RNG.uniform(0.0008, 0.005)\n",
    "    lr = base_lr / (1 + num_hidden_layers * 0.03)  # Lower LR for deeper networks\n",
    "    \n",
    "    dropout = RNG.uniform(0.05, 0.3)  # Slightly higher dropout range\n",
    "\n",
    "    # [num_hidden_layers, *layer_sizes, beta, lr, dropout]\n",
    "    return [num_hidden_layers, *layer_sizes, beta, lr, dropout]\n",
    "\n",
    "\n",
    "def bounded_mutation_with_depth(individual, mu=0, sigma=0.15, indpb=0.2):\n",
    "    \"\"\"Improved mutation with better depth exploration.\"\"\"\n",
    "    num_layers = int(individual[0])\n",
    "\n",
    "    # Mutate number of layers (increased probability)\n",
    "    if RNG.random() < 0.7:  # Increased from 0.15\n",
    "        # Allow both adding and removing layers\n",
    "        change = 1 if RNG.random() < 0.75 else -1 # Can remove, stay same, or add\n",
    "        new_num_layers = max(2, min(8, num_layers + change))\n",
    "\n",
    "        if new_num_layers > num_layers:\n",
    "            # Add a new layer (smaller than previous)\n",
    "            prev_size = individual[num_layers] if num_layers > 0 else 128\n",
    "            new_layer_size = max(32, int(prev_size * 0.7 + RNG.uniform(-10, 10)))\n",
    "            individual.insert(num_layers + 1, new_layer_size)\n",
    "        elif new_num_layers < num_layers:\n",
    "            # Remove a layer (remove from middle preferentially)\n",
    "            if num_layers > 1:\n",
    "                remove_idx = RNG.choice(range(1, min(num_layers + 1, len(individual) - 3)))\n",
    "                individual.pop(remove_idx)\n",
    "\n",
    "        individual[0] = new_num_layers\n",
    "        num_layers = new_num_layers\n",
    "\n",
    "    # Mutate layer sizes with smaller perturbations\n",
    "    for i in range(1, num_layers + 1):\n",
    "        if i < len(individual) - 3 and RNG.random() < indpb:\n",
    "            individual[i] += RNG.normal(mu, sigma * 30)  # Smaller mutations\n",
    "            individual[i] = max(32, min(512, individual[i]))  # Wider range\n",
    "\n",
    "    # Mutate beta (wider range)\n",
    "    if len(individual) > num_layers + 1 and RNG.random() < indpb:\n",
    "        individual[num_layers + 1] += RNG.normal(mu, sigma * 0.08)\n",
    "        individual[num_layers + 1] = max(0.5, min(0.9, individual[num_layers + 1]))\n",
    "\n",
    "    # Mutate learning rate\n",
    "    if len(individual) > num_layers + 2 and RNG.random() < indpb:\n",
    "        individual[num_layers + 2] += RNG.normal(mu, sigma * 0.002)\n",
    "        individual[num_layers + 2] = max(0.0005, min(0.01, individual[num_layers + 2]))\n",
    "\n",
    "    # Mutate dropout\n",
    "    if len(individual) > num_layers + 3 and RNG.random() < indpb:\n",
    "        individual[num_layers + 3] += RNG.normal(mu, sigma * 0.05)\n",
    "        individual[num_layers + 3] = max(0.0, min(0.3, individual[num_layers + 3]))\n",
    "\n",
    "    return (individual,)\n",
    "\n",
    "\n",
    "def crossover_with_depth(ind1, ind2):\n",
    "    \"\"\"Improved crossover for variable-length individuals with diversity preservation.\"\"\"\n",
    "    num_layers1 = int(ind1[0])\n",
    "    num_layers2 = int(ind2[0])\n",
    "\n",
    "    # Sometimes choose depth randomly to maintain diversity\n",
    "    if RNG.random() < 0.3:  # 30% chance for random depth choice\n",
    "        chosen_depth = RNG.choice([num_layers1, num_layers2])\n",
    "    else:\n",
    "        # Choose depth more intelligently (favor successful depths)\n",
    "        if hasattr(ind1, \"fitness\") and hasattr(ind2, \"fitness\"):\n",
    "            if ind1.fitness.valid and ind2.fitness.valid:\n",
    "                if ind1.fitness.values[0] > ind2.fitness.values[0]:\n",
    "                    chosen_depth = num_layers1\n",
    "                else:\n",
    "                    chosen_depth = num_layers2\n",
    "            else:\n",
    "                chosen_depth = RNG.choice([num_layers1, num_layers2])\n",
    "        else:\n",
    "            chosen_depth = RNG.choice([num_layers1, num_layers2])\n",
    "\n",
    "    # Create new individuals\n",
    "    new_ind1 = [chosen_depth]\n",
    "    new_ind2 = [chosen_depth]\n",
    "\n",
    "    # Blend layer sizes\n",
    "    for i in range(chosen_depth):\n",
    "        if i < num_layers1 and i < num_layers2:\n",
    "            # Blend existing layers\n",
    "            alpha = RNG.uniform(0.3, 0.7)  # Less extreme blending\n",
    "            size1 = alpha * ind1[1 + i] + (1 - alpha) * ind2[1 + i]\n",
    "            size2 = (1 - alpha) * ind1[1 + i] + alpha * ind2[1 + i]\n",
    "        elif i < num_layers1:\n",
    "            size1 = ind1[1 + i] + RNG.uniform(-10, 10)\n",
    "            size2 = RNG.uniform(32, 256)\n",
    "        elif i < num_layers2:\n",
    "            size1 = RNG.uniform(32, 256)\n",
    "            size2 = ind2[1 + i] + RNG.uniform(-10, 10)\n",
    "        else:\n",
    "            size1 = RNG.uniform(64, 256)\n",
    "            size2 = RNG.uniform(64, 256)\n",
    "\n",
    "        new_ind1.append(max(32, min(512, size1)))\n",
    "        new_ind2.append(max(32, min(512, size2)))\n",
    "\n",
    "    # Blend other parameters\n",
    "    alpha = RNG.uniform(0.3, 0.7)\n",
    "    beta1 = alpha * ind1[num_layers1 + 1] + (1 - alpha) * ind2[num_layers2 + 1]\n",
    "    beta2 = (1 - alpha) * ind1[num_layers1 + 1] + alpha * ind2[num_layers2 + 1]\n",
    "    lr1 = alpha * ind1[num_layers1 + 2] + (1 - alpha) * ind2[num_layers2 + 2]\n",
    "    lr2 = (1 - alpha) * ind1[num_layers1 + 2] + alpha * ind2[num_layers2 + 2]\n",
    "    dropout1 = (\n",
    "        alpha * ind1[num_layers1 + 3] + (1 - alpha) * ind2[num_layers2 + 3]\n",
    "    )\n",
    "    dropout2 = (1 - alpha) * ind1[num_layers1 + 3] + alpha * ind2[\n",
    "        num_layers2 + 3\n",
    "    ]\n",
    "\n",
    "    new_ind1.extend([beta1, lr1, dropout1])\n",
    "    new_ind2.extend([beta2, lr2, dropout2])\n",
    "\n",
    "    ind1[:] = new_ind1\n",
    "    ind2[:] = new_ind2\n",
    "\n",
    "    return ind1, ind2\n",
    "\n",
    "\n",
    "def evaluate_model_with_depth(individual):\n",
    "    \"\"\"Improved evaluation with depth-adaptive training and better debugging.\"\"\"\n",
    "    try:\n",
    "        # Parse individual\n",
    "        num_layers = int(individual[0])\n",
    "        layer_sizes = [\n",
    "            max(64, min(768, round(individual[i])))\n",
    "            for i in range(1, num_layers + 1)\n",
    "        ]\n",
    "        beta = max(0.5, min(0.95, float(individual[num_layers + 1])))\n",
    "        lr = max(0.0003, min(0.008, float(individual[num_layers + 2])))\n",
    "        dropout = max(0.0, min(0.4, float(individual[num_layers + 3])))\n",
    "\n",
    "        # Debug: Print individual parameters\n",
    "        console.print(f\"Training: Layers={layer_sizes}, Beta={beta:.3f}, LR={lr:.4f}, Dropout={dropout:.3f}\")\n",
    "\n",
    "        model = create_snn(layer_sizes, beta, dropout)\n",
    "        model.to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=5e-4,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Depth-adaptive training - deeper networks get more training\n",
    "        max_batches = min(1200, 400 + num_layers * 100)\n",
    "        \n",
    "        # Extended training loop\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # Depth-adaptive gradient clipping\n",
    "            if num_layers > 4:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            elif num_layers > 2:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "        # More comprehensive evaluation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        eval_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "                data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                _, pred = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += (pred == targets).sum().item()\n",
    "\n",
    "                # Evaluate on more samples for better accuracy\n",
    "                if total >= 3000:  # Increased from 2000\n",
    "                    break\n",
    "\n",
    "        accuracy = correct / total\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        avg_eval_loss = eval_loss / (batch_idx + 1)\n",
    "        #commit\n",
    "        # Debug: Print training details\n",
    "        console.print(f\"Training Loss: {avg_train_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}\")\n",
    "        console.print(f\"Correct: {correct}/{total} = {accuracy:.4f}\")\n",
    "\n",
    "        # Small complexity penalty to encourage efficient architectures\n",
    "        complexity_penalty = (num_layers * 0.001 + sum(layer_sizes) * 0.000005)\n",
    "        final_fitness = accuracy\n",
    "\n",
    "        console.print(f\"Final Fitness: {final_fitness:.6f} (Accuracy: {accuracy:.4f}, Penalty: {complexity_penalty:.6f})\")\n",
    "        return (final_fitness,)\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"Evaluation ERROR: {str(e)}\")\n",
    "\n",
    "        return (-1.0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bcb91c",
   "metadata": {},
   "source": [
    "Creating plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(layer_sizes: list[int]) -> int:\n",
    "    # Full layer structure: input (784) + hidden layers + output (10)\n",
    "    full_layers = [784] + layer_sizes + [10]\n",
    "    total = 0\n",
    "    for i in range(len(full_layers) - 1):\n",
    "        input_size = full_layers[i]\n",
    "        output_size = full_layers[i + 1]\n",
    "        total += (input_size * output_size) + output_size  # weights + biases\n",
    "    return total\n",
    "\n",
    "def parse_individual(individual):\n",
    "    \"\"\"Helper function to parse individual parameters.\"\"\"\n",
    "    num_layers = int(individual[0])\n",
    "    layer_sizes = [round(individual[i]) for i in range(1, num_layers + 1)]\n",
    "    beta = individual[num_layers + 1]\n",
    "    lr = individual[num_layers + 2]\n",
    "    dropout = individual[num_layers + 3]\n",
    "    return num_layers, layer_sizes, beta, lr, dropout\n",
    "\n",
    "\n",
    "def print_best_individual(generation, best_individual, fitness) -> None:\n",
    "    \"\"\"Print detailed information about the best individual.\"\"\"\n",
    "    _num_layers, layer_sizes, beta, lr, dropout = parse_individual(\n",
    "        best_individual,\n",
    "    )\n",
    "\n",
    "    console.print(f\"\\n{'=' * 60}\")\n",
    "    console.print(f\"GENERATION {generation} - BEST INDIVIDUAL\")\n",
    "    console.print(f\"{'=' * 60}\")\n",
    "    console.print(f\"Fitness: {fitness:.6f}\")\n",
    "    console.print(\n",
    "        f\"Architecture: 784 → {' → '.join(map(str, layer_sizes))} → 10\",\n",
    "    )\n",
    "    console.print(f\"Beta (β): {beta:.4f}\")\n",
    "    console.print(f\"Learning Rate: {lr:.6f}\")\n",
    "    console.print(f\"Dropout Rate: {dropout:.4f}\")\n",
    "    console.print(\n",
    "    f\"Total Parameters: ~{count_parameters(layer_sizes):,}\",\n",
    ")\n",
    "    console.print(f\"{'=' * 60}\")\n",
    "\n",
    "\n",
    "def plot_evolution_progress(logbook, generation_best) -> None:\n",
    "    \"\"\"Plot evolution progress with multiple metrics.\"\"\"\n",
    "    _fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Extract data\n",
    "    generations = [record[\"gen\"] for record in logbook]\n",
    "    max_fitness = [record[\"max\"] for record in logbook]\n",
    "    avg_fitness = [record[\"avg\"] for record in logbook]\n",
    "    min_fitness = [record[\"min\"] for record in logbook]\n",
    "    std_fitness = [record[\"std\"] for record in logbook]\n",
    "\n",
    "    # Plot 1: Fitness Evolution\n",
    "    ax1.plot(\n",
    "        generations,\n",
    "        max_fitness,\n",
    "        \"r-\",\n",
    "        linewidth=2,\n",
    "        label=\"Max Fitness\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        generations,\n",
    "        avg_fitness,\n",
    "        \"b-\",\n",
    "        linewidth=2,\n",
    "        label=\"Avg Fitness\",\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        generations,\n",
    "        [avg - std for avg, std in zip(avg_fitness, std_fitness, strict=False)],\n",
    "        [avg + std for avg, std in zip(avg_fitness, std_fitness, strict=False)],\n",
    "        alpha=0.3,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Fitness\")\n",
    "    ax1.set_title(\"Evolution Progress - Fitness Over Time\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Best Fitness Trajectory\n",
    "    best_gens = [item[0] for item in generation_best]\n",
    "    best_fits = [item[2] for item in generation_best]\n",
    "    ax2.plot(best_gens, best_fits, \"g-\", linewidth=3, marker=\"*\", markersize=8)\n",
    "    ax2.set_xlabel(\"Generation\")\n",
    "    ax2.set_ylabel(\"Best Fitness\")\n",
    "    ax2.set_title(\"Best Individual Fitness Trajectory\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Annotate improvements\n",
    "    for i in range(1, len(best_fits)):\n",
    "        if best_fits[i] > best_fits[i - 1]:\n",
    "            ax2.annotate(\n",
    "                f\"↑ {best_fits[i]:.4f}\",\n",
    "                xy=(best_gens[i], best_fits[i]),\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                bbox={\n",
    "                    \"boxstyle\": \"round,pad=0.3\",\n",
    "                    \"facecolor\": \"yellow\",\n",
    "                    \"alpha\": 0.7,\n",
    "                },\n",
    "                arrowprops={\n",
    "                    \"arrowstyle\": \"->\",\n",
    "                    \"connectionstyle\": \"arc3,rad=0\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "    # Plot 3: Fitness Distribution\n",
    "    try:\n",
    "        final_gen_data = [\n",
    "            ind.fitness.values[0]\n",
    "            for ind in logbook[-1][\"population\"]\n",
    "            if hasattr(ind, \"fitness\") and ind.fitness.valid\n",
    "        ]\n",
    "    except KeyError:\n",
    "        final_gen_data = [max_fitness[-1], avg_fitness[-1], min_fitness[-1]]\n",
    "\n",
    "    ax3.hist(\n",
    "        final_gen_data,\n",
    "        bins=15,\n",
    "        alpha=0.7,\n",
    "        color=\"purple\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax3.axvline(\n",
    "        np.mean(final_gen_data),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {np.mean(final_gen_data):.4f}\",\n",
    "    )\n",
    "    ax3.set_xlabel(\"Fitness\")\n",
    "    ax3.set_ylabel(\"Frequency\")\n",
    "    ax3.set_title(\"Final Generation Fitness Distribution\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Convergence Analysis\n",
    "    improvement_rate = []\n",
    "    for i in range(1, len(max_fitness)):\n",
    "        rate = max_fitness[i] - max_fitness[i - 1]\n",
    "        improvement_rate.append(rate)\n",
    "\n",
    "    ax4.plot(\n",
    "        generations[1:],\n",
    "        improvement_rate,\n",
    "        \"orange\",\n",
    "        linewidth=2,\n",
    "        marker=\"d\",\n",
    "    )\n",
    "    ax4.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "    ax4.set_xlabel(\"Generation\")\n",
    "    ax4.set_ylabel(\"Fitness Improvement\")\n",
    "    ax4.set_title(\"Convergence Rate (Generation-to-Generation Improvement)\")\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/evolution_progress.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_architecture_evolution(generation_best) -> None:\n",
    "    \"\"\"Plot architecture evolution over generations.\"\"\"\n",
    "    _fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Extract architecture data\n",
    "    generations = []\n",
    "    num_layers_list = []\n",
    "    layer_sizes_by_gen = []\n",
    "    total_params = []\n",
    "\n",
    "    for gen, individual, _fitness in generation_best:\n",
    "        generations.append(gen)\n",
    "        num_layers, layer_sizes, _beta, _lr, _dropout = parse_individual(\n",
    "            individual,\n",
    "        )\n",
    "        num_layers_list.append(num_layers)\n",
    "        layer_sizes_by_gen.append(layer_sizes)\n",
    "\n",
    "        # Calculate approximate parameter count\n",
    "        params = 784 * layer_sizes[0]  # Input to first hidden\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            params += layer_sizes[i] * layer_sizes[i + 1]\n",
    "        params += layer_sizes[-1] * 10  # Last hidden to output\n",
    "        total_params.append(params)\n",
    "\n",
    "    # Plot 1: Number of Layers Evolution\n",
    "    ax1.plot(generations, num_layers_list, \"b-o\", linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Number of Hidden Layers\")\n",
    "    ax1.set_title(\"Architecture Depth Evolution\")\n",
    "    ax1.set_ylim(0.5, 10.5)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Annotate layer changes\n",
    "    for i in range(1, len(num_layers_list)):\n",
    "        if num_layers_list[i] != num_layers_list[i - 1]:\n",
    "            change = \"+\" if num_layers_list[i] > num_layers_list[i - 1] else \"-\"\n",
    "            ax1.annotate(\n",
    "                f\"{change}Layer\",\n",
    "                xy=(generations[i], num_layers_list[i]),\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                bbox={\"boxstyle\": \"round,pad=0.3\", \"facecolor\": \"lightblue\"},\n",
    "                arrowprops={\"arrowstyle\": \"->\"},\n",
    "            )\n",
    "\n",
    "    # Plot 2: Layer Sizes Heatmap\n",
    "    max_layers = max(len(sizes) for sizes in layer_sizes_by_gen)\n",
    "    heatmap_data = np.zeros((len(generations), max_layers))\n",
    "\n",
    "    for i, sizes in enumerate(layer_sizes_by_gen):\n",
    "        for j, size in enumerate(sizes):\n",
    "            heatmap_data[i, j] = size\n",
    "\n",
    "    im = ax2.imshow(\n",
    "        heatmap_data.T,\n",
    "        cmap=\"viridis\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    ax2.set_xlabel(\"Generation\")\n",
    "    ax2.set_ylabel(\"Layer Index\")\n",
    "    ax2.set_title(\"Layer Sizes Evolution Heatmap\")\n",
    "    ax2.set_xticks(range(0, len(generations), max(1, len(generations) // 10)))\n",
    "    ax2.set_xticklabels(generations[:: max(1, len(generations) // 10)])\n",
    "    plt.colorbar(im, ax=ax2, label=\"Layer Size\")\n",
    "\n",
    "    # Plot 3: Total Parameters Evolution\n",
    "    ax3.plot(generations, total_params, \"r-s\", linewidth=2, markersize=6)\n",
    "    ax3.set_xlabel(\"Generation\")\n",
    "    ax3.set_ylabel(\"Total Parameters\")\n",
    "    ax3.set_title(\"Model Complexity Evolution\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Format y-axis to show K/M notation\n",
    "    ax3.yaxis.set_major_formatter(\n",
    "        plt.FuncFormatter(\n",
    "            lambda x, p: f\"{x / 1000:.0f}K\"\n",
    "            if x < 1000000\n",
    "            else f\"{x / 1000000:.1f}M\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Plot 4: Architecture Visualization for Best Individual\n",
    "    best_individual = max(generation_best, key=operator.itemgetter(2))\n",
    "    _, best_layer_sizes, _, _, _ = parse_individual(best_individual[1])\n",
    "\n",
    "    # Create network diagram\n",
    "    ax4.set_xlim(0, 10)\n",
    "    ax4.set_ylim(0, 10)\n",
    "\n",
    "    # Draw layers\n",
    "    layer_positions = np.linspace(\n",
    "        1,\n",
    "        9,\n",
    "        len(best_layer_sizes) + 2,\n",
    "    )  # +2 for input and output\n",
    "    all_sizes = [784, *best_layer_sizes, 10]\n",
    "\n",
    "    max_neurons = max(all_sizes)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(all_sizes)))\n",
    "\n",
    "    for i, (pos, size, color) in enumerate(\n",
    "        zip(layer_positions, all_sizes, colors, strict=False),\n",
    "    ):\n",
    "        # Scale rectangle height based on layer size\n",
    "        height = (size / max_neurons) * 6 + 1\n",
    "        y_center = 5\n",
    "\n",
    "        rect = Rectangle(\n",
    "            (pos - 0.3, y_center - height / 2),\n",
    "            0.6,\n",
    "            height,\n",
    "            facecolor=color,\n",
    "            edgecolor=\"black\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax4.add_patch(rect)\n",
    "\n",
    "        # Add labels\n",
    "        if i == 0:\n",
    "            label = f\"Input\\n{size}\"\n",
    "        elif i == len(all_sizes) - 1:\n",
    "            label = f\"Output\\n{size}\"\n",
    "        else:\n",
    "            label = f\"H{i}\\n{size}\"\n",
    "\n",
    "        ax4.text(\n",
    "            pos,\n",
    "            y_center - height / 2 - 0.5,\n",
    "            label,\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # Draw connections\n",
    "        if i < len(layer_positions) - 1:\n",
    "            ax4.arrow(\n",
    "                pos + 0.3,\n",
    "                y_center,\n",
    "                0.4,\n",
    "                0,\n",
    "                head_width=0.1,\n",
    "                head_length=0.1,\n",
    "                fc=\"gray\",\n",
    "                ec=\"gray\",\n",
    "                alpha=0.5,\n",
    "            )\n",
    "\n",
    "    ax4.set_title(\n",
    "        f\"Best Architecture (Gen {best_individual[0]}, Fitness: {best_individual[2]:.4f})\",\n",
    "    )\n",
    "    ax4.set_xticks([])\n",
    "    ax4.set_yticks([])\n",
    "    ax4.spines[\"top\"].set_visible(False)\n",
    "    ax4.spines[\"right\"].set_visible(False)\n",
    "    ax4.spines[\"bottom\"].set_visible(False)\n",
    "    ax4.spines[\"left\"].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"plots/architecture_evolution.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_beta_evolution(generation_best) -> None:\n",
    "    \"\"\"Plot beta parameter evolution and analysis.\"\"\"\n",
    "    _fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    # Extract parameter data\n",
    "    generations = []\n",
    "    betas = []\n",
    "    learning_rates = []\n",
    "    dropouts = []\n",
    "    fitnesses = []\n",
    "\n",
    "    for gen, individual, fitness in generation_best:\n",
    "        generations.append(gen)\n",
    "        _num_layers, _layer_sizes, beta, lr, dropout = parse_individual(\n",
    "            individual,\n",
    "        )\n",
    "        betas.append(beta)\n",
    "        learning_rates.append(lr)\n",
    "        dropouts.append(dropout)\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    # Plot 1: Beta Evolution\n",
    "    ax1.plot(\n",
    "        generations,\n",
    "        betas,\n",
    "        \"purple\",\n",
    "        linewidth=3,\n",
    "        marker=\"o\",\n",
    "        markersize=8,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Beta Value\")\n",
    "    ax1.set_title(\"Beta Parameter Evolution\")\n",
    "    ax1.set_ylim(0.65, 1.0)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Highlight significant changes\n",
    "    for i in range(1, len(betas)):\n",
    "        if abs(betas[i] - betas[i - 1]) > 0.05:\n",
    "            ax1.annotate(\n",
    "                f\"β={betas[i]:.3f}\",\n",
    "                xy=(generations[i], betas[i]),\n",
    "                xytext=(10, 10),\n",
    "                textcoords=\"offset points\",\n",
    "                bbox={\n",
    "                    \"boxstyle\": \"round,pad=0.3\",\n",
    "                    \"facecolor\": \"yellow\",\n",
    "                    \"alpha\": 0.7,\n",
    "                },\n",
    "                arrowprops={\"arrowstyle\": \"->\"},\n",
    "            )\n",
    "\n",
    "    # Plot 2: Learning Rate Evolution\n",
    "    ax2.plot(\n",
    "        generations,\n",
    "        learning_rates,\n",
    "        \"green\",\n",
    "        linewidth=3,\n",
    "        marker=\"s\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax2.set_xlabel(\"Generation\")\n",
    "    ax2.set_ylabel(\"Learning Rate\")\n",
    "    ax2.set_title(\"Learning Rate Evolution\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.ticklabel_format(style=\"scientific\", axis=\"y\", scilimits=(0, 0))\n",
    "\n",
    "    # Plot 3: Parameter Correlation with Fitness\n",
    "    # Create scatter plot with color coding for generations\n",
    "    scatter = ax3.scatter(\n",
    "        betas,\n",
    "        fitnesses,\n",
    "        c=generations,\n",
    "        cmap=\"viridis\",\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    ax3.set_xlabel(\"Beta Value\")\n",
    "    ax3.set_ylabel(\"Fitness\")\n",
    "    ax3.set_title(\"Beta vs Fitness Correlation\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax3, label=\"Generation\")\n",
    "\n",
    "    # Add trend line\n",
    "    z = np.polyfit(betas, fitnesses, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax3.plot(\n",
    "        betas,\n",
    "        p(betas),\n",
    "        \"r--\",\n",
    "        alpha=0.8,\n",
    "        linewidth=2,\n",
    "        label=f\"Trend: y={z[0]:.2f}x+{z[1]:.2f}\",\n",
    "    )\n",
    "    ax3.legend()\n",
    "\n",
    "    # Plot 4: All Parameters Over Time\n",
    "    # Normalize parameters to [0, 1] for comparison\n",
    "    epsilon = 1e-10  # Tiny value to prevent division by zero\n",
    "    norm_betas = [\n",
    "    (b - min(betas)) / (max(betas) - min(betas) + epsilon) \n",
    "    for b in betas\n",
    "    ]\n",
    "    norm_lrs = [\n",
    "        (lr - min(learning_rates)) / (max(learning_rates) - min(learning_rates))\n",
    "        for lr in learning_rates\n",
    "    ]\n",
    "    norm_dropouts = [\n",
    "        (d - min(dropouts)) / (max(dropouts) - min(dropouts))\n",
    "        if max(dropouts) > min(dropouts)\n",
    "        else 0.5\n",
    "        for d in dropouts\n",
    "    ]\n",
    "\n",
    "    ax4.plot(\n",
    "        generations,\n",
    "        norm_betas,\n",
    "        \"purple\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        label=\"Beta (normalized)\",\n",
    "    )\n",
    "    ax4.plot(\n",
    "        generations,\n",
    "        norm_lrs,\n",
    "        \"green\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        label=\"Learning Rate (normalized)\",\n",
    "    )\n",
    "    ax4.plot(\n",
    "        generations,\n",
    "        norm_dropouts,\n",
    "        \"orange\",\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        label=\"Dropout (normalized)\",\n",
    "    )\n",
    "\n",
    "    ax4.set_xlabel(\"Generation\")\n",
    "    ax4.set_ylabel(\"Normalized Parameter Value\")\n",
    "    ax4.set_title(\"All Parameters Evolution (Normalized)\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/beta_evolution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_layers_evolution(logbook, generation_best) -> None:\n",
    "    \"\"\"Plot the evolution of layer counts in the population.\"\"\"\n",
    "    _fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Extract data\n",
    "    generations = [record[\"gen\"] for record in logbook]\n",
    "    avg_layers = [record[\"avg_layers\"] for record in logbook]\n",
    "\n",
    "    # Best individual layers\n",
    "    best_layers = [ind[1][0] for ind in generation_best]\n",
    "\n",
    "    # Plot 1: Average layers vs best individual layers\n",
    "    ax1.plot(\n",
    "        generations,\n",
    "        avg_layers,\n",
    "        \"b-\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        label=\"Population Average\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        generations,\n",
    "        best_layers,\n",
    "        \"r-\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        label=\"Best Individual\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Number of Layers\")\n",
    "    ax1.set_title(\"Layer Count Evolution\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Layer distribution in final population\n",
    "    final_pop = logbook[-1][\"population\"]\n",
    "    final_layers = [ind[0] for ind in final_pop]\n",
    "\n",
    "    unique_layers = sorted(set(final_layers))\n",
    "    counts = [final_layers.count(l) for l in unique_layers]\n",
    "\n",
    "    ax2.bar(unique_layers, counts, color=\"purple\", alpha=0.7)\n",
    "    ax2.set_xlabel(\"Number of Layers\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.set_title(\"Final Population Layer Distribution\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"plots/layers_evolution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_comprehensive_summary_plot(logbook, generation_best) -> None:\n",
    "    \"\"\"Create a comprehensive summary plot with key metrics.\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Extract all data\n",
    "    generations = [record[\"gen\"] for record in logbook]\n",
    "    max_fitness = [record[\"max\"] for record in logbook]\n",
    "    avg_fitness = [record[\"avg\"] for record in logbook]\n",
    "\n",
    "    gen_data = []\n",
    "    fitness_data = []\n",
    "    beta_data = []\n",
    "    lr_data = []\n",
    "    layers_data = []\n",
    "    param_counts = []\n",
    "\n",
    "    for gen, individual, fitness in generation_best:\n",
    "        gen_data.append(gen)\n",
    "        fitness_data.append(fitness)\n",
    "        num_layers, layer_sizes, beta, lr, _dropout = parse_individual(\n",
    "            individual,\n",
    "        )\n",
    "        beta_data.append(beta)\n",
    "        lr_data.append(lr)\n",
    "        layers_data.append(num_layers)\n",
    "\n",
    "        # Calculate parameters\n",
    "        params = 784 * layer_sizes[0]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            params += layer_sizes[i] * layer_sizes[i + 1]\n",
    "        params += layer_sizes[-1] * 10\n",
    "        param_counts.append(params)\n",
    "\n",
    "    # Main fitness plot (spans 2 columns)\n",
    "    ax_main = fig.add_subplot(gs[0, :2])\n",
    "    ax_main.plot(\n",
    "        generations,\n",
    "        max_fitness,\n",
    "        \"r-\",\n",
    "        linewidth=3,\n",
    "        label=\"Best Individual\",\n",
    "        marker=\"o\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax_main.plot(\n",
    "        generations,\n",
    "        avg_fitness,\n",
    "        \"b-\",\n",
    "        linewidth=2,\n",
    "        label=\"Population Avg\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax_main.plot(\n",
    "        generations,\n",
    "        backprop_accuracies[:len(generations)],\n",
    "        \"g-\",\n",
    "        linewidth=3,\n",
    "        label=\"PyTorch\",\n",
    "        marker=\"*\",\n",
    "        markersize=10,\n",
    "    )\n",
    "    ax_main.set_xlabel(\"Generation\", fontsize=12)\n",
    "    ax_main.set_ylabel(\"Fitness\", fontsize=12)\n",
    "    ax_main.set_title(\n",
    "        \"Evolution Summary - Fitness Progression\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax_main.legend(fontsize=11)\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "\n",
    "    # Best individual info (right column, top)\n",
    "    ax_info = fig.add_subplot(gs[0, 2])\n",
    "    ax_info.axis(\"off\")\n",
    "    best_idx = fitness_data.index(max(fitness_data))\n",
    "    best_gen = gen_data[best_idx]\n",
    "    best_fitness = fitness_data[best_idx]\n",
    "    best_individual = generation_best[best_idx][1]\n",
    "\n",
    "    _, best_layers, best_beta, best_lr, best_dropout = parse_individual(\n",
    "        best_individual,\n",
    "    )\n",
    "\n",
    "    info_text = f\"\"\"BEST INDIVIDUAL\n",
    "\n",
    "Generation: {best_gen}\n",
    "Fitness: {best_fitness:.6f}\n",
    "\n",
    "Architecture:\n",
    "• Layers: {len(best_layers)}\n",
    "• Sizes: {best_layers}\n",
    "• Parameters: ~{param_counts[best_idx]:,}\n",
    "\n",
    "Parameters:\n",
    "• Beta: {best_beta:.4f}\n",
    "• Learning Rate: {best_lr:.6f}\n",
    "• Dropout: {best_dropout:.4f}\"\"\"\n",
    "\n",
    "    ax_info.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        info_text,\n",
    "        transform=ax_info.transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox={\n",
    "            \"boxstyle\": \"round,pad=0.5\",\n",
    "            \"facecolor\": \"lightblue\",\n",
    "            \"alpha\": 0.8,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Architecture depth evolution\n",
    "    ax_depth = fig.add_subplot(gs[1, 0])\n",
    "    ax_depth.plot(\n",
    "        gen_data,\n",
    "        layers_data,\n",
    "        \"purple\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax_depth.set_xlabel(\"Generation\")\n",
    "    ax_depth.set_ylabel(\"Number of Layers\")\n",
    "    ax_depth.set_title(\"Architecture Depth\")\n",
    "    ax_depth.grid(True, alpha=0.3)\n",
    "    ax_depth.set_ylim(1.5, 4.5)\n",
    "\n",
    "    # Parameter count evolution\n",
    "    ax_params = fig.add_subplot(gs[1, 1])\n",
    "    ax_params.plot(\n",
    "        gen_data,\n",
    "        param_counts,\n",
    "        \"orange\",\n",
    "        linewidth=2,\n",
    "        marker=\"^\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax_params.set_xlabel(\"Generation\")\n",
    "    ax_params.set_ylabel(\"Parameters\")\n",
    "    ax_params.set_title(\"Model Complexity\")\n",
    "    ax_params.grid(True, alpha=0.3)\n",
    "    ax_params.yaxis.set_major_formatter(\n",
    "        plt.FuncFormatter(lambda x, p: f\"{x / 1000:.0f}K\"),\n",
    "    )\n",
    "\n",
    "    # Beta evolution\n",
    "    ax_beta = fig.add_subplot(gs[1, 2])\n",
    "    ax_beta.plot(\n",
    "        gen_data,\n",
    "        beta_data,\n",
    "        \"red\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax_beta.set_xlabel(\"Generation\")\n",
    "    ax_beta.set_ylabel(\"Beta Value\")\n",
    "    ax_beta.set_title(\"Beta Parameter\")\n",
    "    ax_beta.grid(True, alpha=0.3)\n",
    "    ax_beta.set_ylim(0.65, 1.0)\n",
    "\n",
    "    # Performance vs complexity scatter\n",
    "    ax_scatter = fig.add_subplot(gs[2, 0])\n",
    "    scatter = ax_scatter.scatter(\n",
    "        param_counts,\n",
    "        fitness_data,\n",
    "        c=gen_data,\n",
    "        cmap=\"viridis\",\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"black\",\n",
    "    )\n",
    "    ax_scatter.set_xlabel(\"Parameter Count\")\n",
    "    ax_scatter.set_ylabel(\"Fitness\")\n",
    "    ax_scatter.set_title(\"Performance vs Complexity\")\n",
    "    ax_scatter.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax_scatter, label=\"Generation\")\n",
    "\n",
    "    # Learning rate evolution\n",
    "    ax_lr = fig.add_subplot(gs[2, 1])\n",
    "    ax_lr.plot(\n",
    "        gen_data,\n",
    "        lr_data,\n",
    "        \"green\",\n",
    "        linewidth=2,\n",
    "        marker=\"d\",\n",
    "        markersize=6,\n",
    "    )\n",
    "    ax_lr.set_xlabel(\"Generation\")\n",
    "    ax_lr.set_ylabel(\"Learning Rate\")\n",
    "    ax_lr.set_title(\"Learning Rate Evolution\")\n",
    "    ax_lr.grid(True, alpha=0.3)\n",
    "    ax_lr.ticklabel_format(style=\"scientific\", axis=\"y\", scilimits=(0, 0))\n",
    "\n",
    "    # Improvement rate\n",
    "    ax_improve = fig.add_subplot(gs[2, 2])\n",
    "    improvement_rate = [0]  # First generation has no improvement\n",
    "    improvement_rate.extend(\n",
    "        fitness_data[i] - fitness_data[i - 1]\n",
    "        for i in range(1, len(fitness_data))\n",
    "    )\n",
    "\n",
    "    colors = [\"red\" if x < 0 else \"green\" for x in improvement_rate[1:]]\n",
    "    ax_improve.bar(gen_data[1:], improvement_rate[1:], color=colors, alpha=0.7)\n",
    "    ax_improve.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "    ax_improve.set_xlabel(\"Generation\")\n",
    "    ax_improve.set_ylabel(\"Fitness Improvement\")\n",
    "    ax_improve.set_title(\"Generation-to-Generation Progress\")\n",
    "    ax_improve.grid(True, alpha=0.3)\n",
    "\n",
    "    # Final statistics summary\n",
    "    ax_stats = fig.add_subplot(gs[3, :])\n",
    "    ax_stats.axis(\"off\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_improvement = (\n",
    "        fitness_data[-1] - fitness_data[0] if len(fitness_data) > 1 else 0\n",
    "    )\n",
    "    avg_improvement = (\n",
    "        total_improvement / len(fitness_data) if len(fitness_data) > 1 else 0\n",
    "    )\n",
    "    best_improvement_gen = (\n",
    "        improvement_rate.index(max(improvement_rate[1:])) + 1\n",
    "        if len(improvement_rate) > 1\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    convergence_gen = len(fitness_data)\n",
    "    for i in range(len(fitness_data) - 5, 0, -1):\n",
    "        if abs(fitness_data[i] - fitness_data[-1]) > 0.001:\n",
    "            convergence_gen = i + 5\n",
    "            break\n",
    "\n",
    "    stats_text = f\"\"\"EVOLUTION STATISTICS\n",
    "\n",
    "Total Generations: {len(fitness_data)}                    Total Improvement: {total_improvement:.6f}                    Average per Generation: {avg_improvement:.6f}\n",
    "\n",
    "Best Generation: {best_gen}                    Convergence: ~Gen {convergence_gen}                    Best Improvement: Gen {best_improvement_gen} (+{max(improvement_rate[1:]) if len(improvement_rate) > 1 else 0:.6f})\n",
    "\n",
    "Architecture Range: {min(layers_data)}-{max(layers_data)} layers          Parameter Range: {min(param_counts):,}-{max(param_counts):,}          Beta Range: {min(beta_data):.3f}-{max(beta_data):.3f}\"\"\"\n",
    "\n",
    "    ax_stats.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        stats_text,\n",
    "        transform=ax_stats.transAxes,\n",
    "        fontsize=12,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"center\",\n",
    "        bbox={\n",
    "            \"boxstyle\": \"round,pad=1\",\n",
    "            \"facecolor\": \"lightyellow\",\n",
    "            \"alpha\": 0.8,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"SNN Evolution Comprehensive Analysis\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.98,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"plots/comprehensive_evolution_summary.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdede3",
   "metadata": {},
   "source": [
    "Set up  DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61141b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def get_num_layers(individual):\n",
    "    return len(individual[0])\n",
    "\n",
    "def create_individual():\n",
    "    \"\"\"Create an individual and wrap it in creator.Individual.\"\"\"\n",
    "    ind_data = create_individual_with_depth()\n",
    "    return creator.Individual(ind_data)\n",
    "\n",
    "\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_model_with_depth)\n",
    "toolbox.register(\"mate\", crossover_with_depth)\n",
    "toolbox.register(\n",
    "    \"mutate\",\n",
    "    bounded_mutation_with_depth,\n",
    "    mu=0,\n",
    "    sigma=0.1,\n",
    "    indpb=0.15,\n",
    ")\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=POP_SIZE // 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb514c",
   "metadata": {},
   "source": [
    "Running the evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evolution...\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "GENERATION 0 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.819501\n",
      "Architecture: 784 → 237 → 170 → 10\n",
      "Beta (β): 0.7480\n",
      "Learning Rate: 0.003974\n",
      "Dropout Rate: 0.0105\n",
      "Total Parameters: ~500,817\n",
      "============================================================\n",
      "\n",
      "Gen 0 Stats:\n",
      " gen\tnevals\tavg     \tmax     \tmin      \tstd     \n",
      "0  \t400   \t0.491303\t0.819501\t0.0690844\t0.224451\n",
      "\n",
      "--- Processing Generation 1 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 1 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.827071\n",
      "Architecture: 784 → 244 → 160 → 10\n",
      "Beta (β): 0.7997\n",
      "Learning Rate: 0.003889\n",
      "Dropout Rate: 0.1736\n",
      "Total Parameters: ~495,972\n",
      "============================================================\n",
      "Gen 1 \n",
      "Stats: 1  \t266   \t0.725701\t0.827071\t0.385623 \t0.0836032\n",
      "\n",
      "--- Processing Generation 2 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 2 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.828989\n",
      "Architecture: 784 → 252 → 173 → 10\n",
      "Beta (β): 0.9322\n",
      "Learning Rate: 0.004020\n",
      "Dropout Rate: 0.0393\n",
      "Total Parameters: ~530,265\n",
      "============================================================\n",
      "Gen 2 \n",
      "Stats: 2  \t276   \t0.777489\t0.828989\t0.706    \t0.0257494\n",
      "\n",
      "--- Processing Generation 3 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 3 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.831121\n",
      "Architecture: 784 → 217 → 149 → 10\n",
      "Beta (β): 0.8325\n",
      "Learning Rate: 0.003746\n",
      "Dropout Rate: 0.0838\n",
      "Total Parameters: ~436,160\n",
      "============================================================\n",
      "Gen 3 \n",
      "Stats: 3  \t259   \t0.785405\t0.831121\t0.721391 \t0.0232898\n",
      "\n",
      "--- Processing Generation 4 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 4 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 4 \n",
      "Stats: 4  \t292   \t0.788202\t0.840294\t0.736134 \t0.0234483\n",
      "\n",
      "--- Processing Generation 5 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 5 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 5 \n",
      "Stats: 5  \t300   \t0.787942\t0.840294\t0.727993 \t0.0236934\n",
      "\n",
      "--- Processing Generation 6 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 6 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 6 \n",
      "Stats: 6  \t283   \t0.791218\t0.840294\t0.731353 \t0.0231459\n",
      "\n",
      "--- Processing Generation 7 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 7 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 7 \n",
      "Stats: 7  \t279   \t0.79347 \t0.840294\t0.728619 \t0.0238895\n",
      "\n",
      "--- Processing Generation 8 ---\n",
      "\n",
      "============================================================\n",
      "GENERATION 8 - BEST INDIVIDUAL\n",
      "============================================================\n",
      "Fitness: 0.840294\n",
      "Architecture: 784 → 239 → 164 → 10\n",
      "Beta (β): 0.8620\n",
      "Learning Rate: 0.003795\n",
      "Dropout Rate: 0.0936\n",
      "Total Parameters: ~494,361\n",
      "============================================================\n",
      "Gen 8 \n",
      "Stats: 8  \t293   \t0.791268\t0.840294\t0.731631 \t0.0250147\n",
      "\n",
      "--- Processing Generation 9 ---\n"
     ]
    }
   ],
   "source": [
    "def run_evolution():\n",
    "    pop_size = POP_SIZE\n",
    "    pop = toolbox.population(POP_SIZE)\n",
    "    hof = tools.HallOfFame(POP_SIZE // 4)\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"std\", np.std)\n",
    "\n",
    "    # Add a new statistic for average layers\n",
    "    stats_layers = tools.Statistics(\n",
    "        operator.itemgetter(0),\n",
    "    )  # ind[0] is the number of layers\n",
    "    stats_layers.register(\"avg\", np.mean)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"nevals\", *stats.fields]\n",
    "\n",
    "    # Store best individuals from each generation\n",
    "    generation_best = []\n",
    "    # Store populations for each generation\n",
    "    populations = []\n",
    "    # Store average layers per generation\n",
    "    avg_layers_per_gen = []\n",
    "\n",
    "    console.print(\"Starting Evolution...\")\n",
    "    console.print(\"=\" * 80)\n",
    "\n",
    "    # Initial evaluation\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses, strict=False):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    hof.update(pop)\n",
    "    populations.append(pop)  # Store initial population\n",
    "\n",
    "    # Calculate and store average layers for generation 0\n",
    "    avg_layers = stats_layers.compile(pop)[\"avg\"]\n",
    "    avg_layers_per_gen.append(avg_layers)\n",
    "\n",
    "    # Find and display best individual of generation 0\n",
    "    best_ind = tools.selBest(pop, 1)[0]\n",
    "    generation_best.append((0, best_ind.copy(), best_ind.fitness.values[0]))\n",
    "    print_best_individual(0, best_ind, best_ind.fitness.values[0])\n",
    "\n",
    "    record = stats.compile(pop)\n",
    "    logbook.record(gen=0, nevals=len(pop), **record)\n",
    "    console.print(f\"\\nGen 0 Stats:\\n {logbook.stream}\")\n",
    "    console.print(f\"Average layers in population: {avg_layers:.2f}\")\n",
    "\n",
    "    # Evolution loop\n",
    "    for gen in range(1, 31):\n",
    "        console.print(f\"\\n--- Processing Generation {gen} ---\")\n",
    "\n",
    "        # Selection and reproduction\n",
    "        offspring = tools.selTournament(\n",
    "            individuals=pop,\n",
    "            k=POP_SIZE // 2,\n",
    "            tournsize=POP_SIZE // 4,\n",
    "        )\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "        # Crossover\n",
    "        for child1, child2 in zip(\n",
    "            offspring[::2],\n",
    "            offspring[1::2],\n",
    "            strict=False,\n",
    "        ):\n",
    "            if RNG.random() < 0.6:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        # Mutation\n",
    "        for mutant in offspring:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "        # Evaluation\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses, strict=False):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Elitism: Keep best individuals\n",
    "        pop[:] = tools.selTournament(\n",
    "            individuals=pop + offspring,\n",
    "            k=POP_SIZE,\n",
    "            tournsize=POP_SIZE // 4,\n",
    "        )\n",
    "\n",
    "        hof.update(pop)\n",
    "        populations.append(pop.copy())  # Store current population\n",
    "\n",
    "        # Calculate and store average layers for this generation\n",
    "        avg_layers = stats_layers.compile(pop)[\"avg\"]\n",
    "        avg_layers_per_gen.append(avg_layers)\n",
    "\n",
    "        # Find and display best individual of current generation\n",
    "        best_ind = tools.selBest(pop, 1)[0]\n",
    "        generation_best.append((\n",
    "            gen,\n",
    "            best_ind.copy(),\n",
    "            best_ind.fitness.values[0],\n",
    "        ))\n",
    "        print_best_individual(gen, best_ind, best_ind.fitness.values[0])\n",
    "\n",
    "        record = stats.compile(pop)\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        console.print(f\"Gen {gen} \\nStats: {logbook.stream}\")\n",
    "        console.print(f\"Average layers in population: {avg_layers:.2f}\")\n",
    "\n",
    "        # Early stopping if no improvement\n",
    "        if gen > 10 and logbook[-1][\"max\"] == logbook[-5][\"max\"]:\n",
    "            console.print(\n",
    "                f\"\\nEarly stopping at generation {gen} due to no improvement\",\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Add population information to logbook\n",
    "    for i, entry in enumerate(logbook):\n",
    "        entry[\"population\"] = populations[i]\n",
    "        entry[\"avg_layers\"] = avg_layers_per_gen[\n",
    "            i\n",
    "        ]  # Add average layers to logbook\n",
    "\n",
    "    # Plot the average layers evolution\n",
    "    plot_layers_evolution(logbook, generation_best)\n",
    "\n",
    "    # Final summary\n",
    "    console.print(\"\\n\" + \"=\" * 80)\n",
    "    console.print(\"EVOLUTION COMPLETE - FINAL SUMMARY\")\n",
    "    console.print(\"=\" * 80)\n",
    "\n",
    "    # Print overall best\n",
    "    best_overall = max(generation_best, key=operator.itemgetter(2))\n",
    "    best_gen, best_ind, best_fitness = best_overall\n",
    "\n",
    "    console.print(f\"\\nOVERALL BEST INDIVIDUAL (from Generation {best_gen}):\")\n",
    "    print_best_individual(\"FINAL\", best_ind, best_fitness)\n",
    "\n",
    "    # Print evolution summary\n",
    "    console.print(\"\\nEvolution Summary:\")\n",
    "    console.print(f\"- Total Generations: {gen}\")\n",
    "    console.print(f\"- Population Size: {pop_size}\")\n",
    "    console.print(f\"- Best Fitness Achieved: {best_fitness:.6f}\")\n",
    "    console.print(f\"- Best Found in Generation: {best_gen}\")\n",
    "\n",
    "    # Generate all plots\n",
    "    console.print(\"\\nGenerating plots...\")\n",
    "\n",
    "    console.print(\"1. Evolution Progress Plot...\")\n",
    "    plot_evolution_progress(logbook, generation_best)\n",
    "\n",
    "    console.print(\"2. Architecture Evolution Plot...\")\n",
    "    plot_architecture_evolution(generation_best)\n",
    "\n",
    "    console.print(\"3. Beta Evolution Plot...\")\n",
    "    plot_beta_evolution(generation_best)\n",
    "\n",
    "    console.print(\"4. Comprehensive Summary Plot...\")\n",
    "    create_comprehensive_summary_plot(logbook, generation_best)\n",
    "\n",
    "    console.print(\"All plots saved successfully!\")\n",
    "\n",
    "    return pop, logbook, hof, generation_best\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    population, logbook, hall_of_fame, best_individuals = run_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
