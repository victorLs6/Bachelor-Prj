{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee312f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "import math\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_data = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6988dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn(hidden1, hidden2, beta):\n",
    "    class SNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # Define linear layers\n",
    "            self.fc1 = nn.Linear(28*28, hidden1)\n",
    "            self.lif1 = snn.Leaky(beta=beta)\n",
    "            self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "            self.lif2 = snn.Leaky(beta=beta)\n",
    "            self.fc3 = nn.Linear(hidden2, 10)\n",
    "            self.lif3 = snn.Leaky(beta=beta)\n",
    "\n",
    "        def forward(self, x, num_steps=10):\n",
    "            # Initialize membrane potentials\n",
    "            mem1 = self.lif1.init_leaky()\n",
    "            mem2 = self.lif2.init_leaky()\n",
    "            mem3 = self.lif3.init_leaky()\n",
    "            spk_out = 0\n",
    "            for _ in range(num_steps):\n",
    "                cur1 = self.fc1(x.view(x.size(0), -1))\n",
    "                spk1, mem1 = self.lif1(cur1, mem1)\n",
    "                cur2 = self.fc2(spk1)\n",
    "                spk2, mem2 = self.lif2(cur2, mem2)\n",
    "                cur3 = self.fc3(spk2)\n",
    "                spk3, mem3 = self.lif3(cur3, mem3)\n",
    "                spk_out += spk3\n",
    "            return spk_out / num_steps\n",
    "    return SNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounded_mutation(individual, bounds, mu=0, sigma=0.2, indpb=0.2):\n",
    "    \"\"\"Custom mutation that keeps values within specified bounds\"\"\"\n",
    "    for i, (low, high) in enumerate(bounds):\n",
    "        if random.random() < indpb:\n",
    "            # Apply Gaussian mutation\n",
    "            individual[i] += random.gauss(mu, sigma * (high - low))\n",
    "            # Clip to bounds\n",
    "            individual[i] = max(low, min(high, individual[i]))\n",
    "    return individual,\n",
    "\n",
    "# --- 4. Evaluation Function using CrossEntropyLoss ---\n",
    "def evaluate_model(individual):\n",
    "    # Cast and sanitize hyperparameters with bounds checking\n",
    "    hidden1 = max(64, min(256, int(round(individual[0]))))\n",
    "    hidden2 = max(64, min(256, int(round(individual[1]))))\n",
    "    beta = max(0.5, min(0.99, float(individual[2])))\n",
    "    lr = max(0.0001, min(0.01, float(individual[3])))\n",
    "\n",
    "    model = create_snn(hidden1, hidden2, beta)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training (short for speed)\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx >= 20:\n",
    "            break\n",
    "\n",
    "    # Validation on subset\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, pred = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            if total >= 1000:\n",
    "                break\n",
    "    accuracy = correct / total\n",
    "    return (accuracy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual(individual, ax, color='blue', alpha=0.7):\n",
    "    \"\"\"Plot a single individual's parameters as a bar chart\"\"\"\n",
    "    params = ['Hidden1', 'Hidden2', 'Beta', 'LRÃ—1000']\n",
    "    values = [individual[0], individual[1], individual[2], individual[3]*1000]  # Scale LR for visibility\n",
    "    \n",
    "    bars = ax.bar(params, values, color=color, alpha=alpha)\n",
    "    ax.set_ylim(0, 300)\n",
    "    ax.set_ylabel('Parameter Value')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "def plot_population(population, generation, min_fitness, max_fitness):\n",
    "    \"\"\"Plot population diversity and fitness distribution\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Extract parameters and fitness\n",
    "    hidden1_vals = [ind[0] for ind in population]\n",
    "    hidden2_vals = [ind[1] for ind in population]\n",
    "    beta_vals = [ind[2] for ind in population]\n",
    "    lr_vals = [ind[3] for ind in population]\n",
    "    fitness_vals = [ind.fitness.values[0] if ind.fitness.valid else 0 for ind in population]\n",
    "    \n",
    "    # 1. Parameter scatter plots with fitness coloring\n",
    "    scatter1 = ax1.scatter(hidden1_vals, hidden2_vals, c=fitness_vals, \n",
    "                          cmap='viridis', s=100, alpha=0.7, vmin=min_fitness, vmax=max_fitness)\n",
    "    ax1.set_xlabel('Hidden Layer 1 Size')\n",
    "    ax1.set_ylabel('Hidden Layer 2 Size')\n",
    "    ax1.set_title(f'Gen {generation}: Layer Sizes vs Fitness')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Beta vs Learning Rate\n",
    "    scatter2 = ax2.scatter(beta_vals, lr_vals, c=fitness_vals, \n",
    "                          cmap='viridis', s=100, alpha=0.7, vmin=min_fitness, vmax=max_fitness)\n",
    "    ax2.set_xlabel('Beta (Leak Factor)')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_title(f'Gen {generation}: Beta vs Learning Rate')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fitness distribution\n",
    "    ax3.hist(fitness_vals, bins=max(3, len(population)//2), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax3.axvline(np.mean(fitness_vals), color='red', linestyle='--', label=f'Mean: {np.mean(fitness_vals):.3f}')\n",
    "    ax3.axvline(np.max(fitness_vals), color='green', linestyle='--', label=f'Max: {np.max(fitness_vals):.3f}')\n",
    "    ax3.set_xlabel('Fitness (Accuracy)')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title(f'Gen {generation}: Fitness Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Best individual parameters\n",
    "    best_idx = np.argmax(fitness_vals)\n",
    "    best_individual = population[best_idx]\n",
    "    plot_individual(best_individual, ax4, color='gold')\n",
    "    ax4.set_title(f'Gen {generation}: Best Individual (Acc: {fitness_vals[best_idx]:.3f})')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter1, ax=ax4)\n",
    "    cbar.set_label('Fitness (Accuracy)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcf9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution_progress(logbook):\n",
    "    \"\"\"Plot evolution statistics over generations\"\"\"\n",
    "    gen = logbook.select(\"gen\")\n",
    "    avg_fitness = logbook.select(\"avg\")\n",
    "    max_fitness = logbook.select(\"max\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Fitness evolution\n",
    "    ax1.plot(gen, avg_fitness, 'b-', label='Average Fitness', linewidth=2, marker='o')\n",
    "    ax1.plot(gen, max_fitness, 'r-', label='Maximum Fitness', linewidth=2, marker='s')\n",
    "    ax1.fill_between(gen, avg_fitness, max_fitness, alpha=0.2)\n",
    "    ax1.set_xlabel('Generation')\n",
    "    ax1.set_ylabel('Fitness (Accuracy)')\n",
    "    ax1.set_title('Evolution Progress: Fitness Over Generations')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Improvement rate\n",
    "    improvement = np.diff(max_fitness)\n",
    "    ax2.bar(gen[1:], improvement, alpha=0.7, color=['green' if x > 0 else 'red' for x in improvement])\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax2.set_xlabel('Generation')\n",
    "    ax2.set_ylabel('Fitness Improvement')\n",
    "    ax2.set_title('Generation-to-Generation Improvement')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_architecture_evolution(populations_history):\n",
    "    \"\"\"Plot how architecture parameters evolved over time\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    generations = list(range(len(populations_history)))\n",
    "    param_names = ['Hidden Layer 1', 'Hidden Layer 2', 'Beta (Leak Factor)', 'Learning Rate']\n",
    "    param_indices = [0, 1, 2, 3]\n",
    "    \n",
    "    for idx, (ax, param_name, param_idx) in enumerate(zip(axes.flat, param_names, param_indices)):\n",
    "        # Extract parameter values for each generation\n",
    "        for gen, population in enumerate(populations_history):\n",
    "            values = [ind[param_idx] for ind in population if ind.fitness.valid]\n",
    "            fitness = [ind.fitness.values[0] for ind in population if ind.fitness.valid]\n",
    "            \n",
    "            # Scatter plot with fitness coloring\n",
    "            scatter = ax.scatter([gen] * len(values), values, c=fitness, \n",
    "                              cmap='viridis', alpha=0.6, s=50)\n",
    "        \n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel(param_name)\n",
    "        ax.set_title(f'Evolution of {param_name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar to the last subplot\n",
    "    cbar = plt.colorbar(scatter, ax=axes.flat[-1])\n",
    "    cbar.set_label('Fitness (Accuracy)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61141b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define bounds for each parameter\n",
    "BOUNDS = [(64, 256), (64, 256), (0.5, 0.99), (0.0001, 0.01)]\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"hidden1\", random.uniform, 64, 256)\n",
    "toolbox.register(\"hidden2\", random.uniform, 64, 256)\n",
    "toolbox.register(\"beta\", random.uniform, 0.5, 0.99)\n",
    "toolbox.register(\"lr\", random.uniform, 0.0001, 0.01)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.hidden1, toolbox.hidden2, toolbox.beta, toolbox.lr), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_model)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", bounded_mutation, bounds=BOUNDS, mu=0, sigma=0.1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "261f3cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg    \tmax     \n",
      "0  \t6     \t0.71875\t0.777344\n",
      "1  \t5     \t0.735514\t0.777344\n",
      "2  \t1     \t0.750163\t0.777344\n",
      "3  \t2     \t0.764974\t0.777344\n",
      "4  \t4     \t0.731445\t0.777344\n",
      "5  \t4     \t0.708984\t0.777344\n",
      "6  \t3     \t0.720215\t0.777344\n",
      "7  \t1     \t0.74821 \t0.777344\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid learning rate: -0.033257832242173886",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, hof[\u001b[32m0\u001b[39m].fitness.values[\u001b[32m0\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mrun_evolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_evolution\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m stats.register(\u001b[33m\"\u001b[39m\u001b[33mavg\u001b[39m\u001b[33m\"\u001b[39m, np.mean)\n\u001b[32m      6\u001b[39m stats.register(\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, np.max)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m pop, log = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43meaSimple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngen\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                               \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalloffame\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest individual:\u001b[39m\u001b[33m\"\u001b[39m, hof[\u001b[32m0\u001b[39m])\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, hof[\u001b[32m0\u001b[39m].fitness.values[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/deap/algorithms.py:173\u001b[39m, in \u001b[36meaSimple\u001b[39m\u001b[34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[39m\n\u001b[32m    171\u001b[39m invalid_ind = [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind.fitness.valid]\n\u001b[32m    172\u001b[39m fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvalid_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitnesses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(individual)\u001b[39m\n\u001b[32m      9\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m model.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m optimizer = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss_fn = nn.CrossEntropyLoss()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Training (short for speed)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/torch/optim/adam.py:58\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor lr must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0.0\u001b[39m <= lr:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0.0\u001b[39m <= eps:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid epsilon value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Invalid learning rate: -0.033257832242173886"
     ]
    }
   ],
   "source": [
    "def run_evolution():\n",
    "    pop = toolbox.population(n=6)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.3, ngen=10,\n",
    "                                   stats=stats, halloffame=hof, verbose=True)\n",
    "    print(\"\\nBest individual:\", hof[0])\n",
    "    print(\"Accuracy:\", hof[0].fitness.values[0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_evolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
